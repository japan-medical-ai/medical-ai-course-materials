{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![colab-logo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/japan-medical-ai/medical-ai-course-materials/blob/master/notebooks/Introduction_to_ML_libs.ipynb)\n",
    "\n",
    "# 機械学習ライブラリの基礎\n",
    "\n",
    "ここでは，代表的な機械学習アルゴリズムの紹介とチューニングのポイントを数学的な背景と合わせて紹介していきます．\n",
    "機械学習の考え方を身に着ける練習として，**単回帰分析**と**重回帰分析**のアルゴリズムを数式と一緒に考えていきましょう．これらを学ぶことで微分と線形代数，統計の使い方が見えてくると思います．重回帰分析は次章で紹介するニューラルネットワークでもその考え方のベースになるところが多いため，しっかりと数式を理解しておきましょう．\n",
    "\n",
    "## 単回帰分析\n",
    "\n",
    "まずはじめに，機械学習の中でも最も基礎的な単回帰分析について説明します．機械学習アルゴリズムは，教師あり学習と教師なし学習に大別され，単回帰分析は教師あり学習の一種です．教師あり学習の典型的な問題として，10や0.1のように数値（厳密には連続値）を予測する**回帰**と，赤ワイン or 白ワインのようにカテゴリを予測する**分類**があります．単回帰分析はその名の通り，**回帰**を取り扱う手法で，ひとつの入力変数からひとつの出力変数を予測する機械学習アルゴリズムです．\n",
    "\n",
    "### 問題設定\n",
    "\n",
    "例として，**家賃**を予測する問題を考えることにします．ここでは，家賃が出力変数 $y$ となります．\n",
    "次に，**入力変数として何を採用するか**を考えます．機械学習では，データをもとに学習を行いますが，一番最初にどのデータを使って何を予測させるかは人間側で決めるべき問題です．そのため，入力変数として何を採用するかは，人間側の経験に依存します．家賃の予測では，たとえば部屋の広さ，駅からの距離，犯罪発生率などを，入力変数として検討することができます．ここでは部屋の広さを入力変数 $x$ として採用することにしましょう．実際には，複数の入力変数候補があった際に，それらすべてを扱うことができるようなモデル化が一般的ですが，それはこの次の重回帰分析以降で紹介することにします．\n",
    "\n",
    "機械学習のアルゴリズムでは，どの手法も大きく分けて次の3ステップで成り立っています．\n",
    "\n",
    "- Step1: モデルを決める\n",
    "- Step2: 目的関数を決める\n",
    "- Step3: 最適なパラメータを求める\n",
    "\n",
    "参考書を読む際は，「どのようなモデルを用い（定式化を行い）」，「目的関数をどのように決めて」，「どのようにパラメータを最適化するのか」，という3点を意識していきましょう．\n",
    "\n",
    "### Step1. モデルを決める\n",
    "\n",
    "まずStep1では**モデル**を決めます．このモデルとは，出力変数 $y$ と入力変数 $x$ の関係性を**定式化**したものです．家賃の予測値を $y$ とした際に，どのように定式化すればうまく予測することができるのでしょうか．このモデル設計は現在は人手で行うのが一般的であり，機械が自動的に決めてくれるわけではありません（最近ではAutoMLなど，モデルも自動決定する研究も進展してきています）\n",
    "\n",
    "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/01.png)\n",
    "\n",
    "例えば，与えられたデータセットにおいて，家賃と部屋の広さの関係性が次のようになっているとしましょう．この場合，部屋が広くなるほど，家賃が高くなるという関係がみられ．予測のために直線を引くのが妥当にみえます．\n",
    "\n",
    "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/02.png)\n",
    "\n",
    "直線の式は $y=ax+b$ で表され，ここで$a$ は傾き，$b$ は切片とよばれるパラメータです ．今回は，直線を引くことが適切であると判断し，Step1のモデルは\n",
    "$$\n",
    "y = wx + b\n",
    "$$\n",
    "\n",
    "とします．傾き $a$ の箇所が $w$ となっていますが，一般的に機械学習では，傾きの箇所を**重み (weight)** $w$, 切片 $b$ の箇所を**バイアス (bias)** $b$ で表現することが多いため，ほかの参考書が読みやすいように記号も一般的なものに統一しておきましょう．\n",
    "\n",
    "単回帰分析では，このように直線 $y = wx + b$ と決めて，その重みとバイアスの値をデータにうまくフィットするように調整していきます．この調整すべき変数のことを**パラメータ**と呼びます．今回は $w$ と $b$ がパラメータとなります．\n",
    "\n",
    "多くの機械学習ではこのようなパラメータで特徴付けられたモデルを使い，与えられたデータセットにあうように最適なパラメータを求めることが目標となります．ここでデータセットは，部屋の広さ $x$ と**教師データ**となる家賃 $t$ の組からなるデータの集まりです（本解説では，機械学習による予測値を $y$ ，教師データとして与えるものを $t$ ，と使い分けているため覚えておいてください）．データセットは $\\mathcal{D} = \\{x_n, t_n\\}_{n=1}^{N}$ として表されることもあります．ここで，添え字 $n$ ($n=1,2,\\ldots,N$) は $n$ 番目の物件という意味であり，$N$ は全体の物件数のことです．この $N$ を**サンプル数**と呼ぶため覚えておきましょう．\n",
    "\n",
    "ここで，この後の計算を楽に進めるために，話は本題から少し逸れますが，**データの中心化**というテクニックを紹介します．下図に示すように，部屋の広さと家賃は両方とも正の値であるため，左のグラフのような形になります．そして，中心化では，**平均を0**とした中央に配置するような変換の処理を施します．この中心化はどのアルゴリズムでも前処理として行うことが一般的です．厳密には前章で紹介した中心化込みのスケーリングがよく用いられます．\n",
    "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/03.png)\n",
    "\n",
    "この中心化の処理自体はそれほど難しいものではないのですが，なぜこのような処理を行う必要があるのでしょうか．それは下図の通り，データの中心化によってバイアス $b$ が0となり，$y_{c} = wx_{c}$ とバイアス成分をなしで表現することができ，調整すべきパラメータを**2つから1つ**に減らすことができるためです．そして，これにより，人間側での手計算が楽になるということが目的です．\n",
    "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/04.png)\n",
    "\n",
    "さて，データの中心化の目的が明確となったところで，このデータの中心化の処理が難しければ，あまり意味がなくなってしまうのですが，何かを簡単にするために，複雑な処理を挟んでしまっては本末転倒です．しかし，データの中心化はいたってシンプルであり，入出力の平均をデータの全体から引くだけです．つまり，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{c} &= x - \\bar{x} \\\\\n",
    "t_{c} &= t - \\bar{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となる．例えば，具体的な数値で見ると，下図の通りです．\n",
    "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/05.png)\n",
    "\n",
    "この処理をプログラムで書くことは容易です．\n",
    "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/06.png)\n",
    "\n",
    "さて，添え字の $c$ に関して，この先も書いていくと表現が冗長となるため，今後はこの添え字 $c$ を省略し，**データの中心化を事前に行っていることを前提**としていきます．この時に単回帰分析のモデルは\n",
    "$$\n",
    "y = wx\n",
    "$$\n",
    "\n",
    "となります．したがって，単回帰分析の**ゴール**は，データセット $\\mathcal{D} = \\{x_n, t_n\\}_{n=1}^{N}$ に基づいて，パラメータ $w$ を**適切**に調整することです．\n",
    "\n",
    "### Step2. 目的関数を決める\n",
    "\n",
    "Step1では決めたゴールには曖昧さが残っていたことに気づかれたでしょうか．それは**適切**という言葉です．一見もっともらしくも聞こえますが，コンピュータで計算させる際には，適切の定義を決めていない中で適切は存在しません．そこで，**適切の定義を決める**必要があり，これを**目的関数**として定めます．分野によっては**評価関数**と呼ばれることもあります．\n",
    "\n",
    "さて，今回はどのように目的関数を決めれば良いでしょうか．微分の時にもすでに紹介していますが，教師データ $t$ と予測値 $y$ の**二乗誤差** $(t-y)^{2}$ が小さければ小さいほど，適切と考えることができるのではないでしょうか．理想的には二乗誤差が 0 となれば，$t = y$ となり完璧な予測といえます．そのため，$n$ 番目の物件に対する教師データ $t_{n}$ と予測値 $y_{n}$ の二乗誤差は\n",
    "$$\n",
    "(t{_n} - y_{n})^{2}\n",
    "$$\n",
    "\n",
    "となります．これを全物件で考慮する必要があるため，最終的な目的関数はその総和をとり，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}&=\\left( t_{1}-y_{1}\\right)^{2}+\\left( t_{2}-y_{2}\\right)^{2}+\\ldots + (t_{N}-y_{N})^{2} \\\\\n",
    "&=\\sum^{N}_{n=1}\\left( t_{n}-y_{n}\\right)^{2}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となります．また，Step1で決めたモデル\n",
    "$$\n",
    "y_{n} = wx_{n}\n",
    "$$\n",
    "\n",
    "を代入すると，目的関数は\n",
    "$$\n",
    "\\mathcal{L}=\\sum^{N}_{n=1}\\left( t_{n}-wx_{n}\\right)^{2}\n",
    "$$\n",
    "\n",
    "とパラメータを含んだ形式で表現することができます．目的関数の中でも，教師データと予測値の差（損失）を考慮したものを**損失関数**と呼びます．二乗誤差の総和は代表的な損失関数であるため，覚えておきましょう．損失関数は常に**最小化**したいというモチベーションでパラメータの最適化を行います．\n",
    "\n",
    "### Step3. 最適なパラメータを求める \n",
    "\n",
    "モデルと目的関数が決まると，あとは目的関数 $\\mathcal{L}$ を最小化するようなパラメータを求めていくだけです．ここで，ある関数を最小化する点を求める方法としては**微分**が使えることをすでに学びました．そのため，微分を利用して**傾き0**となる点を求めていくだけです．\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial }{\\partial w} \\mathcal{L}  &= \\dfrac{\\partial}{\\partial w} { \\sum^{N}_{n=1} ( t_{n}-wx_{n})^{2} }\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで，微分は**線形性**の性質を持っており，現状ではすべての足し算を終えた後に微分を行っているが，これはそれぞれ微分した後に，それを足し算することでも同じ結果であったため，\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial w} \\mathcal{L}=\\sum^{N}_{n=1}\\dfrac {\\partial }{\\partial w}\\left( t_{n}-wx_{n}\\right)^{2}\n",
    "$$\n",
    "\n",
    "が成り立ちます．この微分と総和 $\\sum$ の記号が入れ替わる場面はよくあるので，この理由も含めてしっかりと覚えておきましょう．そして，\n",
    "$$\n",
    "\\dfrac {\\partial }{\\partial w}\\left( t_{n}-wx_{n}\\right)^{2}\n",
    "$$\n",
    "の部分は**合成関数**になっていることがわかる．$u_{n} = t_{n} - wx_{n}$, $f(u_{n}) = u_{n}^{2}$ とおくと，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac {\\partial }{\\partial w}\\left( t_{n}-wx_{n}\\right)^{2} &=  \\dfrac {\\partial }{\\partial w} f(u_{n}) \\\\\n",
    "\\Rightarrow \\dfrac {\\partial }{\\partial w} f(u_{n}) &= \\dfrac {\\partial u_{n}}{\\partial w} \\dfrac{\\partial f(u_{n})}{\\partial w} \\\\\n",
    "&=-x_{n} \\times 2 u_{n}  \\\\\n",
    "&= -2x_{n}( t_{n}-wx_{n} )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が得られる．これより，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial }{\\partial w} \\mathcal{L}\n",
    "&=\\sum^{N}_{n=1}\\dfrac {\\partial }{\\partial w}\\left( t_{n}-wx_{n}\\right)^{2}\n",
    "\\\\&=-\\sum^{N}_{n=1}2x_{n}\\left( t_{n}-wx_{n}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となる．この微分の値が0となるように$w$を求めていくと，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac {\\partial }{\\partial w} \\mathcal{L} &=0\\\\\n",
    "-2\\sum^{N}_{n=1}x_{n}\\left( t_{n}-wx_{n}\\right) &=0\\\\\n",
    "-2 \\sum^{N}_{n=1}x_{n}t_{n} + 2\\sum^{n}_{n=1}wx^{2}_{n}&=0\\\\\n",
    "-2\\sum^{N}_{n=1}x_{n}t_{n}+2w\\sum^{N}_{n=1}x^{2}_{n}&=0\\\\\n",
    "w\\sum^{N}_{n=1}x^{2}_{n}&=\\sum^{n}_{n=1}x_{n}t_{n}\\\\\n",
    "\\Rightarrow w&=\\dfrac {\\displaystyle  \\sum^{N}_{n=1}x_{n}t_{n}}{\\displaystyle  \\sum^{N}_{n=1}x^{2}_{n}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "と求まりました．この求まったパラメータ $w$ を確認すると，与えられたデータセット $\\mathcal{D} = \\{x_n, t_n\\}_{n=1}^{N}$ のみから決定できていることがわかります．最終的なこの式を知っていれば，単回帰分析における学習はプログラミングで1行で書けてしまいますが，この流れを知ってもらうことがこの後に重要になってきます．全体の流れはいかがでしたでしょうか．\n",
    "\n",
    "数式での議論を進めることができ，もう少し具体的なイメージを持つために，例題にあげていた数値例でパラメータ $w$ を求めてみましょう．まずは，データの中心化をおこなうために，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{x} &= \\dfrac{1}{3} (1 + 2 + 3) = 2 \\\\\n",
    "\\bar{t} &= \\dfrac{1}{3}(2 + 3.9 + 6.1) = 4\n",
    "\\end{aligned}\n",
    "$$\n",
    "とそれぞれの平均を求め，各変数に対して前処理として，中心化の処理を施すと，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{1} &= 1 - 2 = -1 \\\\\n",
    "x_{2} &= 2 -2 = 0 \\\\\n",
    "x_{3} &= 3- 2 = 1\\\\\n",
    "t_{1} &= 2 - 4 = -2\\\\\n",
    "t_{2} &= 3.9 - 4 = -0.1\\\\\n",
    "t_{3} &= 6.1 - 4 = 2.1 \n",
    "\\end{aligned}\n",
    "$$\n",
    "となります．そして，中心化後の値を用いて，最適なパラメータ$w$を導出すると，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w &= \\dfrac{\\displaystyle \\sum_{n=1}^{N}x_{n}t_{n}}{\\displaystyle  \\sum_{n=1}^{N}x_{n}^{2}} \\\\\n",
    "&= \\dfrac{x_{1}t_{1} + x_{2}t_{2} + x_{3}t_{3}}{x_{1}^{2} + x_{2}^{2} + x_{3}^{2}} \\\\\n",
    "&= \\dfrac{-1 \\times (-2) + 0 \\times 0.1 + 1 \\times 2.1}{(-1)^{2} + 0^2 + 1^2} \\\\\n",
    "&= 2.05\n",
    "\\end{aligned}\n",
    "$$\n",
    "と求まりました．これで単回帰分析の学習の手順が完了しました．この求まったパラメータを使用したモデルが**学習済みモデル**となります．機械学習は学習済みモデルを使用して**推論**を行うことで初めて活用であることを忘れてはいけません．例えば，新しいサンプル $x_{q}=1.5$ となるデータが新たなサンプルとして与えられた時の推論を行うと，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{c} &= wx_{c} \\\\\n",
    "y_{q} - \\bar{t} &= w(x_{q}-\\bar{x}) \\\\\n",
    "\\Rightarrow y_{q} &= w(x_{q}-\\bar{x}) + \\bar{t} \\\\\n",
    "&= 2.05 \\times (1.5 - 2) + 4 \\\\\n",
    "&= 2.975\n",
    "\\end{aligned}\n",
    "$$\n",
    "のように予測値が求まりました．これが機械学習の一連の手順である．単回帰分析自体は本書の中で最もシンプルな方法であるが，全体像を把握することと，微分の使いどころを把握するために，とても良い学びになったと思います．\n",
    "\n",
    "## 重回帰分析\n",
    "\n",
    "単回帰分析の延長戦上として，複数の入力変数を扱う際にその基礎となるアルゴリズムが重回帰分析です．重回帰分析は単回帰分析と同様に教師あり学習の一種であり，単回帰分析と同じく回帰を行う手法です．問題設定に関しては，ほとんど単回帰分析と同じですが，重回帰分析では入力変数の数が複数となる．つまり，複数の入力変数から複数の出力変数を予測できるアルゴリズムです．\n",
    "\n",
    "### 問題設定\n",
    "\n",
    "ここでは単回帰分析の場合と同様，身近な例で想像がつきやすい**家賃**の予測を考えることとしましょう．つまり家賃が出力変数 $y$ となります．そして，入力変数としては，前回の単回帰分析では考慮しきれていなかった駅からの距離や犯罪発生率なども考慮していきましょう．例えば，部屋の広さ $x_{1}$, 駅からの距離 $x_{2}$, ..., 犯罪発生率 $x_{M}$ のように $M$ 個の入力変数がある前提で話を進めていくこととします．\n",
    "\n",
    "単回帰分析でも紹介しましたが，どの手法も大きく分けて以下の3つのステップで成り立っています．\n",
    "\n",
    "- モデルを決める\n",
    "- 目的関数を決める\n",
    "- 最適なパラメータを求める\n",
    "\n",
    "### Step1. モデルを決める\n",
    "\n",
    "単回帰分析のモデルは，\n",
    "$$\n",
    "y = wx + b\n",
    "$$\n",
    "\n",
    "でした．ここで，$w$ を重み（weight），$b$ をバイアス (bias) と呼びました．重回帰分析では，この式ベースに複数の入力変数へと拡張し，\n",
    "$$\n",
    "y=w_{1}x_{1}+w_{2}x_{2}+\\ldots +w_{M}x_{M}+b\n",
    "$$\n",
    "\n",
    "のような**線形結合**の形で表します．重回帰分析のモデルは総和を使って整理すると，\n",
    "$$\n",
    "y = \\sum_{m=1}^{M} w_{m} x_{m} + b\n",
    "$$\n",
    "\n",
    "のように書くことができます．ただし，線形代数で学んだ内容をを使うと，さらにすっきりと直感的な式で書くことができます．\n",
    "\n",
    "その前に，バイアス $b$ がきれいな規則性に沿っていないため，本題から話がそれますが，単回帰分析と同様に，この取り扱いについて先に考えましょう．単回帰分析では，**データの中心化**によってバイアス $b$ を無視できように式変形を行いました．単回帰分析ではそれによって，求めるべきパラメータの数が $w$ の１つだけになり，手計算の量が減るというメリットがありました．しかし，今回 $b$ が省略できたところで，パラメータの数が$M+1$ 個から $M$ 個に減るだけでほとんどメリットがないことに気づきます．そこで，下記のように，バイアス $b$ を $w$ で表現して，同じ規則性で包含できるようにする．\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y&=w_{1}x_{1}+w_{2}x_{2}+\\ldots +w_{M}x_{M}+b\\\\\n",
    "&=w_{1}x_{1}+w_{2}x_{2}+\\ldots +w_{M}x_{M}+w_{0} x_{0}\\\\\n",
    "&=w_{0}x_{0}+w_{1}x_{1}+\\ldots +w_{M}x_{M}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで，$x_{0}=1$, $w_{0}=b$ です．このようにバイアス $b$ を包含するテクニックは機械学習を学ぶ上でも多く登場するため，覚えておきましょう．そして，この式をベクトルをうまく駆使して整理していくと，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y&=w_{0}x_{0}+w_{1}x_{1}+\\ldots +w_{M}x_{M}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "w_{0} & w_{1} & \\ldots  & w_{n}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "x_{0} \\\\\n",
    "x_{1} \\\\\n",
    "\\vdots  \\\\\n",
    "x_{M}\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\boldsymbol{w}^{T}\\boldsymbol{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "のように，線形結合で表現できました．また，今後取り扱う際には，$\\boldsymbol{x}$ が前に来ているほうが計算上便利であるため，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y&=w_{0}x_{0}+w_{1}x_{1}+\\ldots +w_{M}x_{M}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "x_{0} & x_{1} & \\ldots  & x_{n}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "w_{0} \\\\\n",
    "w_{1} \\\\\n",
    "\\vdots  \\\\\n",
    "w_{M}\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\boldsymbol{x}^{T}\\boldsymbol{w}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "として表現します．これが重回帰分析のモデルです．今回はパラメータとして $M+1$ 個の重み $\\boldsymbol{w}$ を求めていきます．\n",
    "\n",
    "### Step2. 目的関数を決める\n",
    "\n",
    "単回帰分析では，教師データ $t$ と予測値 $y$ の二乗誤差を小さくできるほど，良い予測であると定義して，この**二乗誤差の総和**を目的関数として定めていました．さて，重回帰分析では，これと問題設定が変わるでしょうか．単回帰分析でも重回帰分析でも，家賃 $y$ を求めるという設定は同じであるため，同じ目的関数となるはずです．したがって，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L&=\\left( t_{1}-y_{1}\\right)^{2}+\\left( t_{2}-y_{2}\\right)^{2}+\\ldots + \\left( t_{N}-y_{N}\\right)^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "のように，二乗誤差の総和を単回帰分析同様，目的関数として採用します．単回帰分析では，これを\n",
    "$$\n",
    "\\mathcal{L}=\\sum^{N}_{n=1} ( t_{n} - y_{n})^{2}\n",
    "$$\n",
    "のように，総和の記号を使ってまとめていましたが，ここでも線形代数で学んだテクニックを活かして，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}&=\\left( t_{1}-y_{1}\\right)^{2}+\\left( t_{2}-y_{2}\\right)^{2}+\\ldots + \\left( t_{N}-y_{N}\\right)^{2}\\\\\n",
    "&=\\begin{bmatrix} t_{1} - y_{1} & t_{2}-y_{2} & \\ldots & t_{N}-y_{N} \\end{bmatrix} \\begin{bmatrix}\n",
    "t_{1}-y_{1} \\\\\n",
    "t_{2}-y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "t_{N}-y_{N}\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\left( \\boldsymbol{t}-\\boldsymbol{y}\\right)^{T}\\left( \\boldsymbol{t}-\\boldsymbol{y}\\right) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "のようにベクトルをうまく使って表現できました．また，$\\boldsymbol{y}$ に関して，Step3に入る前に式を整理しておくと，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{y}=\\begin{bmatrix}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{N}\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "\\boldsymbol{x}_{1}^{T}\\boldsymbol{w} \\\\\n",
    "\\boldsymbol{x}_{2}^{T}\\boldsymbol{w} \\\\\n",
    "\\vdots  \\\\\n",
    "\\boldsymbol{x}_{N}^{T}\\boldsymbol{w}\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "\\boldsymbol{x}_{1}^{T} \\\\\n",
    "\\boldsymbol{x}_{2}^{T} \\\\\n",
    "\\vdots  \\\\\n",
    "\\boldsymbol{x}_{N}^{T}\n",
    "\\end{bmatrix}\n",
    "\\boldsymbol{w}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "のように，書くことができます．数式の抽象度が高まり，わかりにくくなってきたため一度展開すると，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{y}&=\n",
    "\\begin{bmatrix}\n",
    "x_{10} & x_{11} & x_{12} & \\ldots  & x_{1M} \\\\\n",
    "x_{20} & x_{21} & x_{22} & \\ldots  & x_{2M} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots  & \\vdots  \\\\\n",
    "x_{N0} & x_{N1} & x_{N{2}} & \\ldots  & x_{NM}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "w_{0} \\\\\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "\\vdots  \\\\\n",
    "w_{M}\n",
    "\\end{bmatrix}\\\\\n",
    "\\boldsymbol{y}&=\\boldsymbol{X}\\boldsymbol{w}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となっています．ここで，行（横）方向がサンプルを表しており，例えば各物件に対応します．列（縦）方向が入力変数を表しており，例えば，部屋の広さ駅からの距離などが入っています．もう少し具体的な数値で考えると，部屋の広さ50m$^{2}$で駅からの距離600m, 犯罪発生率2%のような $n$ 番目の物件の場合，\n",
    "$$\n",
    "\\boldsymbol{x}_{n}^{T} = \\begin{bmatrix}\n",
    "1 & 50 & 600 & \\cdots & 0.02\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "のようにデータが行方向格納されているイメージです．先頭の $1$ はバイアスを包含する際に使用している $x_{0}$ であることに注意しましょう．\n",
    "\n",
    "### Step3. パラメータを最適化する\n",
    "\n",
    "それでは，Step1で定めたモデルのパラメータを，Step2で定めた目的関数を最小化するように決めていきましょう．\n",
    "\n",
    "まずは目的関数に関して，パラメータ $\\boldsymbol{w}$ で表現できるように式変形を行うと，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}&=\\left( \\boldsymbol{t}-\\boldsymbol{y}\\right)^{T}\\left( \\boldsymbol{t}-\\boldsymbol{y}\\right) \\\\\n",
    "&=\\left( \\boldsymbol{t}-\\boldsymbol{X}\\boldsymbol{w}\\right)^{T}\\left( \\boldsymbol{t}-\\boldsymbol{X}\\boldsymbol{w}\\right) \\\\\n",
    "&= \\left\\{ \\boldsymbol{t}^{T}-(\\boldsymbol{X}\\boldsymbol{w})^{T}\\right\\}\\left( \\boldsymbol{t}-\\boldsymbol{X}\\boldsymbol{w}\\right) \\\\\n",
    "&=\\left( \\boldsymbol{t}^{T}-\\boldsymbol{w}^{T}\\boldsymbol{X}^{T}\\right)\\left( \\boldsymbol{t}-\\boldsymbol{X}\\boldsymbol{w}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となります．ここで，転置の公式 $(\\boldsymbol{A}\\boldsymbol{B})^{T} = \\boldsymbol{B}^{T}\\boldsymbol{A}^{T}$ を使っていることに注意しましょう．さらに分配法則を使って展開を進めていくと，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}&=\\boldsymbol{t}^{T}\\boldsymbol{t}-\\boldsymbol{t}^{T}\\boldsymbol{X}\\boldsymbol{w}-\\boldsymbol{w}^{T}\\boldsymbol{X}^{T}\\boldsymbol{t} + \\boldsymbol{w}^{T}\\boldsymbol{X}^{T}\\boldsymbol{X}\\boldsymbol{w}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となります．ここに対して微分をしていくのも良いのですが，さらにもう少し整理することができます．この整理には少しテクニックが必要であり，\n",
    "$$\n",
    "(1)^T = 1\n",
    "$$\n",
    "\n",
    "というように，当然ではありますが，**スカラーは転置しても同じ**であるという性質を持っています．さて，上式の中で出てくる $\\boldsymbol{t}^{T}\\boldsymbol{X}\\boldsymbol{w}$ はスカラー・ベクトル・行列のどれに対応するであろうか．忘れた方は，線形代数の**サイズ感**のパートで確認してください．答えとして，これは**スカラー**です．そのため，\n",
    "$$\n",
    "(\\boldsymbol{t}^{T}\\boldsymbol{X}\\boldsymbol{w})^{T} = \\boldsymbol{t}^{T}\\boldsymbol{X}\\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "が成り立ちます．さらに，転置の公式 $(\\boldsymbol{A}\\boldsymbol{B}\\boldsymbol{C})^T = \\boldsymbol{A}^T\\boldsymbol{B}^T\\boldsymbol{C}^T$ より，\n",
    "$$\n",
    "(\\boldsymbol{t}^{T}\\boldsymbol{X}\\boldsymbol{w})^T = \\boldsymbol{w}^{T} \\boldsymbol{X}^{T} \\boldsymbol{t}\n",
    "$$\n",
    "\n",
    "も成り立ちます．これより，\n",
    "$$\n",
    "(\\boldsymbol{t}^{T}\\boldsymbol{X}\\boldsymbol{w})^{T} = \\boldsymbol{t}^{T}\\boldsymbol{X}\\boldsymbol{w} = \\boldsymbol{w}^{T} \\boldsymbol{X}^{T} \\boldsymbol{t}\n",
    "$$\n",
    "\n",
    "を導くことができ，目的関数を\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}=\\boldsymbol{t}^{T}\\boldsymbol{t}-2\\boldsymbol{t}^{T}\\boldsymbol{X}\\boldsymbol{w} + \\boldsymbol{w}^{T}\\boldsymbol{X}^{T}\\boldsymbol{X}\\boldsymbol{w}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "のように整理することができます．ここで，今回は $\\boldsymbol{w}$ に関する偏微分を行っていくため，ひとまず $\\boldsymbol{w}$ 以外の定数項をまとめていくと，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}&=\\boldsymbol{t}^{T}\\boldsymbol{t}-2\\boldsymbol{t}^{T}\\boldsymbol{X}\\boldsymbol{w}+\\boldsymbol{w}^{T}\\boldsymbol{X}^{T}\\boldsymbol{X}\\boldsymbol{w}\\\\\n",
    "&=\\boldsymbol{t}^{T}\\boldsymbol{t}-2\\left( \\boldsymbol{X}^{T}\\boldsymbol{t}\\right)^{T} \\boldsymbol{w}+\\boldsymbol{w}^{T}\\boldsymbol{X}^{T}\\boldsymbol{X}\\boldsymbol{w} \\\\\n",
    "&=c+\\boldsymbol{b}^{T}\\boldsymbol{w}+\\boldsymbol{w}^{T}\\boldsymbol{A}\\boldsymbol{w} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "のように，線形代数で学んだ $\\boldsymbol{w}$ に関する二次形式（二次関数）で表現することができました．ここで，$\\boldsymbol{A}= \\boldsymbol{X}^{T}\\boldsymbol{X}, \\ \\boldsymbol{b} =-2 \\boldsymbol{X}^{T}\\boldsymbol{t}, \\ c=\\boldsymbol{t}^{T}\\boldsymbol{t}$ であり，$\\boldsymbol{b}$ を転置の形式にした理由は，線形代数で学んだベクトルで微分の公式の形式に合わせるための工夫です．\n",
    "\n",
    "それでは，目的関数を最小化することができるパラメータ $\\boldsymbol{w}$ の求め方を考えましょう．先述の通り，目的関数はパラメータ $\\boldsymbol{w}$に関して二次関数です．例えば，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{w} = \\begin{bmatrix}\n",
    "w_{1} \\\\ w_{2}\n",
    "\\end{bmatrix}, \n",
    "\\boldsymbol{A}=\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix},\\boldsymbol{b}=\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix},c=1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "のように具体的な数値例で考えてみると，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L} &=\n",
    "\\boldsymbol{w}^{T}\\boldsymbol{A}\\boldsymbol{w}+\\boldsymbol{b}^{T}\\boldsymbol{w}+c\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "w_{1} & w_{2}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2}\n",
    "\\end{bmatrix}\n",
    "+\\begin{bmatrix}\n",
    "1 & 2\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2}\n",
    "\\end{bmatrix}+1\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "w_{1} & w_{2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{1}+2w_{2} \\\\\n",
    "3w_{1}+4w_{2}\n",
    "\\end{bmatrix}+w_{1}+2w_{2}+1\\\\\n",
    "&=w_{1}\\left( w_{1}+2w_{2}\\right) +w_{1}\\left( 3w_{1}+4w_{2}\\right) +w _{1}+2w_{2}+1\\\\\n",
    "&=w^{2}_{1}+5w_{1}w_{2}+4w^{2}_{2}+w_{1}+2w_{2}+1 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となり，$w_{1}, w_{2}$に関してそれぞれまとめると，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}\n",
    "&=w^{2}_{1}+\\left( 5w_{2}+1\\right) w_{1} + \n",
    "\\left( 4w^{2}_{2}+2w_{2}+1\\right) \\\\\n",
    "&=4w^{2}_{2}+\\left( 5w_{1}+2\\right) w_{2}+\\left( w^{2}_{1}+w_{1}+1\\right) \\end{aligned}\n",
    "$$\n",
    "\n",
    "のようにそれぞれの二次関数であることがわかります．ただし，$w_{1}$ と $w_{2}$ が**独立である**といった前提もありますが，最初から厳密な前提は数式が複雑になるため，ひとまず置いておくとしましょう．\n",
    "\n",
    "そして，二次関数であれば，下図のような形となります．\n",
    "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/06.png)\n",
    "\n",
    "これを3次元でイメージすると，下図のようになります．\n",
    "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/07.png)\n",
    "\n",
    "そして，各変数で偏微分して傾きが 0 となる点において，目的関数である二乗誤差の総和が最小となる点となります．\n",
    "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/2/08.png)\n",
    "\n",
    "この例では，$w_{1}$ と $w_{2}$ の２つのパラメータの場合で考えましたが，これは $w_{0}$, $w_{1}$, $w_{2}$, $\\ldots$, $w_{M}$ の場合でも同様に考えることができ，目的関数が最小となる点は\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\dfrac {\\partial }{\\partial w_{0}}\\mathcal{L}=0\\\\\n",
    "\\dfrac {\\partial }{\\partial w_{1}}\\mathcal{L}=0\\\\\n",
    "\\ \\ \\ \\ \\ \\vdots \\\\\n",
    "\\dfrac {\\partial }{\\partial w_{M}}\\mathcal{L}=0\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "となり，これをまとめると，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "\\dfrac {\\partial}{\\partial w_{0}} \\mathcal{L} \\\\\n",
    "\\dfrac {\\partial}{\\partial w_{1}} \\mathcal{L} \\\\\n",
    "\\vdots  \\\\\n",
    "\\dfrac {\\partial}{\\partial w_{M}} \\mathcal{L} \\\\\n",
    "\\end{bmatrix}&=\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots  \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\Rightarrow \\dfrac {\\partial}{\\partial \\boldsymbol{w}} \\mathcal{L} &= \\boldsymbol{0} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "のようにベクトルでの微分として表されます．あとは，上式を満たすように $\\boldsymbol{w}$ を決めていくだけです．下記の計算にはベクトルでの微分をはじめとして，線形代数で学んだ内容をフル活用しているため，計算途中がわからなくなった場合は，線形代数のパートを確認しながら進めてみましょう．\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac {\\partial }{\\partial \\boldsymbol{w}}\\mathcal{L} =\\dfrac {\\partial }{\\partial \\boldsymbol{w}}\\left( c+\\boldsymbol{b}^{T}\\boldsymbol{w}+\\boldsymbol{w}^{T}\\boldsymbol{A}\\boldsymbol{w}\\right)\n",
    "= \\boldsymbol{0}\\\\\n",
    "\\dfrac {\\partial }{\\partial \\boldsymbol{w}}\\left( c\\right) +\\dfrac {\\partial }{\\partial \\boldsymbol{w}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{w}\\right) +\\dfrac {\\partial }{\\partial \\boldsymbol{w}}\\left( \\boldsymbol{w}^{T}\\boldsymbol{A}\\boldsymbol{w}\\right) \n",
    "=\\boldsymbol{0}\\\\\n",
    "\\boldsymbol{0}+\\boldsymbol{b}+\\left( \\boldsymbol{A}+\\boldsymbol{A}^{T}\\right) \\boldsymbol{w} =\\boldsymbol{0}\\\\\n",
    "-2\\boldsymbol{X}^{T}\\boldsymbol{t}+\\left\\{ \\boldsymbol{X}^{T}\\boldsymbol{X} + \\left( \\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{T}\\right\\} \\boldsymbol{w}\n",
    "=\\boldsymbol{0}\\\\\n",
    "-2\\boldsymbol{X}^{T}\\boldsymbol{t}+2\\boldsymbol{X}^{T}\\boldsymbol{X}\\boldsymbol{w}=\\boldsymbol{0}\\\\\n",
    "\\boldsymbol{X}^{T}\\boldsymbol{X}\\boldsymbol{w}=\\boldsymbol{X}^{T}\\boldsymbol{t}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで，両辺に左側から $\\left( \\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}$ をかけると，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left( \\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{X} \\boldsymbol{w} =\\left( \\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{t} \\\\\n",
    "\\boldsymbol{I}\\boldsymbol{w}=\\left( \\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{t} \\\\\n",
    "\\boldsymbol{w}=\\left( \\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "となり，最適なパラメータ $\\boldsymbol{w}$ が求まりました．ここで，$\\boldsymbol{I}$ は単位行列です．このように，最適なパラメータが与えられているデータセット $\\boldsymbol{X}, \\boldsymbol{t}$ で求まりました．また，式変形の際に気を付ける点として，\n",
    "$$\n",
    "\\boldsymbol{w} = \\dfrac{\\boldsymbol{X}^{T}\\boldsymbol{t}}{\\boldsymbol{X}^{T}\\boldsymbol{X}}\n",
    "$$\n",
    "のような分数にはならないことに注意しましょう．これは行列の計算には割り算がないためです．そのため，逆行列を使って行列積のみで計算できるように工夫しています．\n",
    "\n",
    "また，もうひとつよくある間違いとして，\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{X}^{T}\\boldsymbol{X}\\boldsymbol{w}&=\\boldsymbol{X}^{T}\\boldsymbol{t}\\\\\n",
    "\\left( \\boldsymbol{X}^{T}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{X}\\boldsymbol{w}&=\\left( \\boldsymbol{X}^{T}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{t}\\\\\n",
    "\\boldsymbol{X}\\boldsymbol{w}&=\\boldsymbol{t}\\\\\n",
    "\\boldsymbol{X}^{-1}\\boldsymbol{X}\\boldsymbol{w}&=\\boldsymbol{X}^{-1}\\boldsymbol{t}\\\\\n",
    "\\boldsymbol{w}&=\\boldsymbol{X}^{-1}\\boldsymbol{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "のように式変形できないのかといった質問もあります．しかし，これは一般的には成立しません．その理由として，線形代数の章で説明した逆行列を持つための条件として，**正方行列であること**が満たされないためです．バイアス $\\boldsymbol{b}$ を $\\boldsymbol{w}$ に包含することを無視する場合 $\\boldsymbol{X} \\in \\mathcal{R}^{N \\times M}$ であり，バイアスの包含を考慮する場合は $\\boldsymbol{X} \\in \\mathcal{R}^{N \\times (M+1)}$ です．一般的に，サンプル数 $N$ と入力変数の数 $M$ は等しくないため，$\\boldsymbol{X}$は正方行列ではなく，逆行列をもちません．それに対し，例えば，$\\boldsymbol{X} \\in \\mathcal{R}^{N \\times M}$ の場合，$\\boldsymbol{X}^{T}\\boldsymbol{X} \\in \\mathcal{R}^{M\\times M}$ となり，サンプル数 $N$ に依存することなく，常に正方行列となります．また，逆行列が求まるためにはもう少し厳密な条件があるのですが，さらに詳しく知りたい方は**フルランク**と調べてみてください．\n",
    "\n",
    "推論の際は学習で得られたパラメータ $\\boldsymbol{w}$ を用いて，\n",
    "\n",
    "$$\n",
    "y_{q} = \\boldsymbol{w}^{T}\\boldsymbol{x}_{q}\n",
    "$$\n",
    "\n",
    "のように計算することで予測値が得られます．\n",
    "\n",
    "\n",
    "\n",
    "## Numpyによる実装\n",
    "\n",
    "それでは，重回帰分析を例にPythonで線形代数を用いた実装を行っていきましょう．Pythonには**Numpy**と呼ばれる線形代数を簡単に扱えるライブラリが存在し，これを使うことが実質標準となっています．次の章で紹介するChainerの中でもNumpyは多用されており，ディープラーニングを学ぶための第一歩として，まずはNumpyの使い方を習得することが需要です．\n",
    "\n",
    "Pythonの文法に関しては把握していることを前提に進めています．具体的には，変数（数値・文字列，リスト，タプル，辞書），制御構文（for，if），関数，クラスを理解している必要があります．\n",
    "\n",
    "重回帰分析では，最終的に最適なパラメータ $\\boldsymbol{w}$ が\n",
    "$$\n",
    "\\boldsymbol{w}=\\left( \\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "で求まりました．この最適なパラメータを求めるためには，以下の5つを扱える必要があります．\n",
    "\n",
    "- ベクトルの定義\n",
    "- 行列の定義\n",
    "- 転置\n",
    "- 行列積\n",
    "- 逆行列\n",
    "\n",
    "具体的に，以下のようなデータセットが与えられているケースを想定してみましょう．\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 2 & 5 \\\\\n",
    "1 & 3 & 4 \\\\  \n",
    "1 & 5 & 9 \n",
    "\\end{bmatrix}, \\\n",
    "\\boldsymbol{t} = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 5 \\\\ 6 \\\\ 8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "それぞれの実装について，見ていきましょう．まずは，Numpyの読み込みから始めます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベクトルの定義は以下のように行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[1], [5], [6], [8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [5]\n",
      " [6]\n",
      " [8]]\n"
     ]
    }
   ],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つぎに，行列の定義も行いましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 5],\n",
    "    [1, 3, 4],\n",
    "    [1, 5, 9]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [1 2 5]\n",
      " [1 3 4]\n",
      " [1 5 9]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それで，Xの転置を行おう．Numpyの`array`で定義されている場合，`.T`をつけるだけで転置ができる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]\n",
      " [2 2 3 5]\n",
      " [3 5 4 9]]\n"
     ]
    }
   ],
   "source": [
    "print(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "縦と横が入れ替わっていることを確認できた．\n",
    "\n",
    "次に，行列積は以下のように `np.dot` によって実現できる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = np.dot(X.T, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4  12  21]\n",
      " [ 12  42  73]\n",
      " [ 21  73 131]]\n"
     ]
    }
   ],
   "source": [
    "print(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つぎに，この逆行列を求めるには，`np.linalg.inv` を用いる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_inv = np.linalg.inv(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76530612 -0.39795918 -0.06122449]\n",
      " [-0.39795918  0.84693878 -0.40816327]\n",
      " [-0.06122449 -0.40816327  0.24489796]]\n"
     ]
    }
   ],
   "source": [
    "print(XX_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで重回帰分析のために必要な演算がそろった．最適なパラメータを求めると，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = np.dot(X.T, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20]\n",
      " [ 70]\n",
      " [124]]\n"
     ]
    }
   ],
   "source": [
    "print(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.dot(XX_inv, Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14285714]\n",
      " [ 0.71428571]\n",
      " [ 0.57142857]]\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにパラメータ $\\boldsymbol{w}$ が求まりました．Numpyを使うことで，数式とプログラミングの間にあったギャップを簡単に埋めることができました．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learnによる本格的な実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重回帰分析であればNumpyで簡単に実装することができましたが，実務で本格的に使用していく機械学習アルゴリズムは複雑なことが多く，初学者が一から書くには難しいことが多いです．そこで，Pythonでは**Scikit-learn**と呼ばれる機械学習用のフレームワークが公開されており，初学者でも簡単に機械学習を扱うことができます．\n",
    "\n",
    "まずは重回帰分析をScikit-learnによって実装してみましょう．\n",
    "\n",
    "### Scikit-learn 基礎編\n",
    "\n",
    "Scikit-learnは`sklearn`という名前で呼び出すことができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たとえば，重回帰分析を使用する場合は以下のように呼び出します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは[公式のリファレンス](http://scikit-learn.org/)を見ながらどこに格納されているか調べても良いのですが，「重回帰分析 Scikit-learn」と検索して，実例のソースコードを見るほうが早い場合が多いです．\n",
    "\n",
    "重回帰分析のアルゴリズムがクラスとして定義されており，まずはインスタンス化を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この一行で重回帰分析を使用するための準備が完了です．そして，パラメータの学習は以下のように行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後にどのような結果が得られたかの検証も一行で行えます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076926"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回帰では**決定係数**と呼ばれる指標で，分類の場合は**精度**が自動的に計算されるようになっています．\n",
    "このように，Scikit-learnでは，とても簡単なインターフェースでやり取りができるようになっている．Scikit-learnの良い点は最初にアルゴリズムを決めてしまえば，一からの実装が難しいアルゴリズムでも，`.fit`で学習，`.score`で検証が行える点です．\n",
    "\n",
    "また，アルゴリズムによって内容は多少異なりますが，パラメータもインスタンス変数として格納されているため，学習後に確認することができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.71428571, 0.57142857]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パラメータw\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14285714])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# バイアスb\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この例からわかるように，Scikit-learnでは，パラメータとバイアスがそれぞれ準備されているため，入力変数 $\\boldsymbol{X}$ の左端の列に1を格納した変数を入れる必要がありません．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn 応用編\n",
    "\n",
    "Scikit-learnは機械学習の実装を支援する多くの機能を兼ね備えており，基礎編では紹介できていなかった実務では必ず使う機能を紹介します．\n",
    "\n",
    "まず最初にサンプルのデータセットの取り扱いを紹介します．Scikit-learnにサンプルのデータセットがいくつか提供されており，今回はこのデータセットで話を進めていくこととします．今回は`load_boston`というボストン近郊の家賃に関するデータセットを使用しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変数の`boston`には辞書と同じ形式で格納されており，変数の中身を見ながら入力変数と教師データに対応するものを見つけていきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston['data']\n",
    "t = boston['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpyの形式で入力変数と教師データが格納されており，`.shape`を使うことで行と列の数を確認できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つぎに，**訓練データ**と**検証データ**の分割を行う必要があります．先ほどは，すべてのデータを訓練に使って，すべてのデータを検証に使っていました．しかし，応用を考えた場合，これで良いのでしょうか．\n",
    "\n",
    "たとえば，受験勉強のために10年分の過去問を購入した場合，10年分を使って勉強して，実力試し用にまた同じ10年分を使うでしょうか．そうではなく，例えば7年分を勉強用に使って，残りの3年分を実力試し用に置いておくはずです．機械学習もこの考え方と全く同じで，勉強用を訓練データ，実力試しを検証データとして分けて用います．このように分割して検証することを**ホールドアウト法**と呼びます．\n",
    "\n",
    "Scikit-learnではもちろんこの訓練用と検証用を分割する機能が用意されています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 13)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 13)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引数の`test_size`は検証用に使うデータの比率であり，0.3と指定すると全体の30%が検証データとなります．また，`random_state`は乱数のシードであり，再現性を確保するためのものです．なぜ乱数が登場するかというと，前から70%を訓練用，残りを検証用とするのではなく，全体からランダムに選択した70%を訓練用，残り30%を検証用と並びにかかわらず選択できるようになっているためです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして，パラメータの学習には訓練データを用います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検証を行う場合は，訓練データと検証データの両方に対してチェックしておきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7644563391821222"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練データ\n",
    "model.score(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673528086534723"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証データ\n",
    "model.score(X_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検証データだけでなく，訓練データでも検証することに意味があります．\n",
    "実務を行うときには，以下のような結果のどれかが得られ，それによって改善していくための施策が変わってきます．\n",
    "\n",
    "|訓練データ|検証データ|結果|\n",
    "|:--|:--|:--|\n",
    "|×|×|アンダーフィッティング|\n",
    "|〇|×|オーバーフィッティング|\n",
    "|〇|〇|ＯＫ|\n",
    "\n",
    "訓練データに対して悪く，検証データで良好な結果が得られている場合はたまたまであり，再現性が低いため，今回は対象外としています．\n",
    "\n",
    "**アンダーフィッティング**の場合は，現状の機械学習アルゴリズムでうまくデータの特徴を捉えられていないと考えられ，アルゴリズムを変更したり，入力となるデータの特徴を表せるような変換を考えます．逆に**オーバーフィッティング**の時は，そのアルゴリズムである程度特徴を捉えられていることはわかっているため，**ハイパーパラメータ**と呼ばれる各アルゴリズムに固有の値を調整して解決していくことが多いです．望ましい結果が得られない中にも，それぞれの状況を把握し，次に打つべき対策が変わってくるため，訓練データと検証データの両方に対する検証を行うことは重要であることがわかります．．\n",
    "\n",
    "また，Scikit-learnでは，スケーリングも簡単に行うことができる．例えば，平均0，標準偏差1に変換する処理を施す場合の手順は以下の通りです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンス化\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均の分散（標準偏差）を学習\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変換\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20416267 -0.49997924  1.54801583 ...  1.2272573   0.42454294\n",
      "   3.10807269]\n",
      " [-0.38584317  0.34677427 -0.58974728 ...  0.05696346  0.40185312\n",
      "  -0.66643035]\n",
      " [-0.33266283 -0.49997924  1.54801583 ...  1.2272573   0.39846135\n",
      "   0.63936662]\n",
      " ...\n",
      " [-0.38147768 -0.49997924 -0.15303077 ... -0.30312696  0.39659002\n",
      "  -0.30284441]\n",
      " [-0.3720831  -0.49997924 -0.59690657 ... -0.25811566  0.37588849\n",
      "   0.89967717]\n",
      " [-0.38289844 -0.49997924 -1.00641779 ... -0.84326258  0.42454294\n",
      "   0.31822262]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.39152624 -0.49997924 -1.12239824 ... -0.70822867  0.17086147\n",
      "  -0.72160487]\n",
      " [ 0.70825498 -0.49997924  1.00534187 ...  0.77714428  0.0648977\n",
      "  -0.41177872]\n",
      " [-0.38588517 -0.49997924  0.4025299  ... -0.93328518  0.38758427\n",
      "  -0.27454978]\n",
      " ...\n",
      " [ 1.6177735  -0.49997924  1.00534187 ...  0.77714428  0.42454294\n",
      "   2.59876943]\n",
      " [-0.34043865 -0.49997924 -0.1687812  ... -0.03305915  0.42454294\n",
      "  -1.11772962]\n",
      " [-0.39601293 -0.49997924 -1.27417512 ...  0.10197476  0.39202867\n",
      "  -1.02294263]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実用的な機械学習アルゴリズムの紹介\n",
    "\n",
    "これまでは重回帰分析の紹介にとどまっていたが，ここからは実務でもよく用いられる機械学習アルゴリズムについて特徴とともに紹介していきます．数式を詳細に紹介していくと長くなりすぎてしまうため，気になったアルゴリズムがあれば，参考図書を見て学びを深めてください．\n",
    "\n",
    "Scikit-learnを使うことによって実装は非常に手軽に行うことができますが，数式を理解していないがゆえに，うまくいかないときの対処法がわからないという問題がありますが，この問題につまずかないように，実務でチューニングを行うハイパーパラメータとその探索する値の相場も併せて紹介していきます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "SVMは実用的によく使われる手法の一つであり，入出力間の非線形性も捉えることができます．$y=x^2$ や $y = \\sin(x)$ のように，$y=wx + b$ の直線ではないモデル化を行うことができます．ただし，非線形なモデルの定式化は非常に難しい問題の１つです．なぜなら，$y=wx^2$ が良いのか，$y=w\\sin(x)$ が良いのか，それともその重ね合わせが良いのかと組み合わせの候補が無限に存在するためです．物理現象に基づいて入出力間の関係性が把握できていれば定式化のアイディアも存在しますが，そのような事前知識がある場合は多くないはずです．\n",
    "\n",
    "そこで，SVMでは**カーネルトリック**と呼ばれるテクニックを駆使して，入出力間の関係性が非線形な場合の定式化も可能にしています．この数学は非常に興味深いのですが，初学者には難易度が高いため，ある程度機械学習の数学に慣れてから取り組んで見てください．\n",
    "\n",
    "参考図書：[オンライン機械学習](https://www.amazon.co.jp/%E3%82%AA%E3%83%B3%E3%83%A9%E3%82%A4%E3%83%B3%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E6%B5%B7%E9%87%8E-%E8%A3%95%E4%B9%9F/dp/406152903X/ref=sr_1_1?ie=UTF8&qid=1542261385&sr=8-1&keywords=%E3%82%AA%E3%83%B3%E3%83%A9%E3%82%A4%E3%83%B3%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92)\n",
    "\n",
    "SVMには連続値を予測する**回帰 (Regression)** とカテゴリを予測する**分類 (Classification)** の両方に対応した手法を持っており，それぞれを**，Support Vector Regression (SVR)** と **Support Vector Classification (SVC)** と呼びます．まずは回帰の問題設定で紹介していき，前回のボストン近郊の家賃の予測の例題を取り扱います．\n",
    "\n",
    "#### Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston['data']\n",
    "t = boston['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練と検証データの分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルのインスタンス化，学習\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "model = SVR()\n",
    "model.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14680479454958428"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01018093344367077"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように数式が難しいSVRにおいてもScikit-learnでは重回帰分析のケースとほとんど同じように実装できます．\n",
    "\n",
    "ここで，結果に着目すると，訓練データに対しても検証データに対しても良い結果が得られているとは言い難いです．\n",
    "話題に上がっていたハイパーパラメータを調整すれば良くなるのか，それともSVRではそもそもダメなのかと迷うところです．\n",
    "実はハイパーパラメータの調整の前に，スケーリングを行うことで改善ができることが多く，その効果を見てみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# スケーリング後のデータを使って学習\n",
    "model.fit(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.697669153907031"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5540391127752358"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test_s, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように，大幅に結果を改善することができた．理由は数式を理解しておかないと説明が難しいが，スケーリングの影響を大きく受けるアルゴリズムと受けないアルゴリズムがあり，SVRを含むSVMは影響を受けるアルゴリズムといえます．\n",
    "\n",
    "最後に，さらに精度を高められないかとハイパーパラメータの調整を行いましょう．ハイパーパラメータの調整を行うときにはひとつ注意すべき点があり，訓練データ（train）はパラメータの調整に用いますが，検証データ（test）を見ながらハイパーパラーメータの調整を行うべきでしょうか．検証データはあくまで未知の状態に対する予測性能の検証を行うためのものであるため，ハイパーパラメータの調整に使用してしまうとそれは学習に使ってしまうことになります．\n",
    "\n",
    "そこで，ハイパーパラメータの調整用に**バリデーションデータ**（val）を追加することが一般的です．\n",
    "\n",
    "![](images/2/09.png)\n",
    "\n",
    "また，このバリデーションデータの追加と一緒に導入されるものとして，**交差検証法（クロスバリデーション）**があります．話の始まりとしては，trainとtestを分けた時点で学習に使えるサンプル数が少なくなってしまっている中，さらにvalも分けると学習に使えるサンプル数がさらに減ってしまいます．そうなると，valのサンプル数を少なくしたいのですが，バリデーションに使うサンプル数が少ないと，たまたまうまくいっているのか，どのサンプルに対してもうまくいくのかがわからなくなります．そこで，下図に示すような交差検証法が用いられます．\n",
    "\n",
    "![](images/2/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainとvalの分割を1パターンだけでなく，複数パターン分けて行い，その平均をとる方法です．この分割数 $K$ として，**K-fold Cross Validation (CV)**と呼ばれることが多いため，この名称も覚えておこう．上記の例だと $K=3$ です．\n",
    "\n",
    "それでは，SVRのハイパーパラメータ調整を交差検証法も使いながら行っていきます．Scikit-learnではハイパーパラメータ調整のための機能も`GridSearchCV`という名前で準備されており，**グリッドサーチ**とは各組合せをすべて試す探索方法です．それ以外の方法として，**ランダムサーチ**と**ベイズ最適化**による探索があるが，ここは余裕がでてきた段階でさらに深める内容のひとつとしてほしいトピックのひとつです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調整を行うハイパーパラメータの値の候補\n",
    "param_grid = [\n",
    "    {'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1, 10]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交差検証法を使用したハイパーパラメータの各組合せでの学習\n",
    "model_cv = GridSearchCV(SVR(), param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "model_cv.fit(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交差検証法とハイパーパラメータのグリッドサーチもこれだけで完了です．各ハイパーパラメータでの結果ももちろん確認することができ，最も結果の良かったハイパーパラメータの値を引き継いだモデルの選択もできます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryosu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: -40.88957, std: 12.03388, params: {'C': 1, 'gamma': 0.01},\n",
       " mean: -34.94548, std: 12.18057, params: {'C': 1, 'gamma': 0.1},\n",
       " mean: -72.62060, std: 15.99632, params: {'C': 1, 'gamma': 1},\n",
       " mean: -86.25200, std: 16.38372, params: {'C': 1, 'gamma': 10},\n",
       " mean: -17.67763, std: 6.48783, params: {'C': 10, 'gamma': 0.01},\n",
       " mean: -16.46703, std: 7.03969, params: {'C': 10, 'gamma': 0.1},\n",
       " mean: -43.71719, std: 13.22953, params: {'C': 10, 'gamma': 1},\n",
       " mean: -81.13324, std: 15.21847, params: {'C': 10, 'gamma': 10},\n",
       " mean: -13.83363, std: 3.54540, params: {'C': 100, 'gamma': 0.01},\n",
       " mean: -14.61609, std: 7.20850, params: {'C': 100, 'gamma': 0.1},\n",
       " mean: -37.47299, std: 9.87515, params: {'C': 100, 'gamma': 1},\n",
       " mean: -77.95797, std: 12.36436, params: {'C': 100, 'gamma': 10}]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 結果の確認 (valの対する結果)\n",
    "model_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.01}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最も結果が良かったハイパーパラメータ\n",
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最も結果が良かったハイパーパラメータの値を設定したモデル\n",
    "model = model_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7685336670918761"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test_s, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでがアルゴリズムの実践的な調整です．実際には特徴量の選択や外れ値除去など前処理も込みで行うため，ここまでシンプルに完了できるものではないですが，まずはこの流れを覚えてください．\n",
    "\n",
    "1. スケーリング無　score:0.010\n",
    "2. スケーリング有　score:0.554\n",
    "3. スケーリング＋ハイパーパラメータの調整有　0.7685"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classification (SVC)\n",
    "\n",
    "次に，SVMの分類であるSVCも同様にスケーリングからハイパーパラメータの調整まで行う．分類の例題では，乳がんの患者か否かといったこれもScikit-learn側で準備されているデータセットを使用する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの準備\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer['data']\n",
    "t = breast_cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力変数のスケールは統一されていないことがわかります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データと検証データに分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# スケーリング無で学習\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631578947368421"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分類では精度 (Accuracy) と呼ばれる指標の結果が得られます．例えば，100問中3問間違えると，精度は0.97となります．\n",
    "\n",
    "次にスケーリングを行った後に学習させていきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スケーリング\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# スケーリングしたデータを用いて学習\n",
    "model.fit(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824120603015075"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766081871345029"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test_s, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように精度が大幅に高まったことがわかりました．\n",
    "\n",
    "最後にハイパーパラメータのチューニングも行っていきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調整を行うハイパーパラメータの値の候補\n",
    "param_grid = [\n",
    "    {'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1, 10]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交差検証法を使用したハイパーパラメータの各組合せでの学習\n",
    "model_cv = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "model_cv.fit(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryosu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.96482, std: 0.01272, params: {'C': 1, 'gamma': 0.01},\n",
       " mean: 0.95226, std: 0.01543, params: {'C': 1, 'gamma': 0.1},\n",
       " mean: 0.62814, std: 0.00310, params: {'C': 1, 'gamma': 1},\n",
       " mean: 0.62563, std: 0.00223, params: {'C': 1, 'gamma': 10},\n",
       " mean: 0.97487, std: 0.01972, params: {'C': 10, 'gamma': 0.01},\n",
       " mean: 0.94472, std: 0.02474, params: {'C': 10, 'gamma': 0.1},\n",
       " mean: 0.63065, std: 0.00132, params: {'C': 10, 'gamma': 1},\n",
       " mean: 0.62563, std: 0.00223, params: {'C': 10, 'gamma': 10},\n",
       " mean: 0.94975, std: 0.01981, params: {'C': 100, 'gamma': 0.01},\n",
       " mean: 0.94472, std: 0.02474, params: {'C': 100, 'gamma': 0.1},\n",
       " mean: 0.63065, std: 0.00132, params: {'C': 100, 'gamma': 1},\n",
       " mean: 0.62563, std: 0.00223, params: {'C': 100, 'gamma': 10}]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 結果の確認 (valの対する結果)\n",
    "model_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.01}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最も結果が良かったハイパーパラメータ\n",
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最も結果が良かったハイパーパラメータの値を設定したモデル\n",
    "model = model_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9883040935672515"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test_s, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ハイパーパラメータの調整により，多少とはなりましたが改善することができました．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "つぎに，決定木 (Dicision Tree) のアンサンブル学習であるランダムフォレストです．こちらも実用上良く使われる手法であり，ランダムフォレスト含めた決定木系の手法では入力変数のスケールの違いによる影響はほとんど受けないという特徴を持っています．また，**カテゴリカル変数**と呼ばれる定量評価を行うことが難しい変数（例えば，男性 or 女性）も定量化を気にすることなく扱うことができるメリットがあります．回帰と分類と両方準備されているため，それぞれについて紹介していきましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回帰 (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston['data']\n",
    "t = boston['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練と検証データの分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルのインスタンス化，学習\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9683509759630142"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8244110898822086"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# スケーリング後のデータを使って学習\n",
    "model.fit(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.980773304078463"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7568763837538154"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test_s, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように，スケーリングによる影響はほとんどないこと受けませんでした．\n",
    "\n",
    "また，Random Forest含めた決定木系の手法では，まずは条件分岐させる数である `max_depth` をハイパーパラメータとして調整することが多いため，今回はこちらを調整していきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調整を行うハイパーパラメータの値の候補\n",
    "param_grid = [\n",
    "    {'max_depth': [1, 2, 3, 4, 5, 6]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交差検証法を使用したハイパーパラメータの各組合せでの学習\n",
    "model_cv = GridSearchCV(RandomForestRegressor(), param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "model_cv.fit(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryosu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: -41.89307, std: 9.40057, params: {'max_depth': 1},\n",
       " mean: -25.95305, std: 8.05238, params: {'max_depth': 2},\n",
       " mean: -23.11041, std: 4.68079, params: {'max_depth': 3},\n",
       " mean: -17.92487, std: 4.42161, params: {'max_depth': 4},\n",
       " mean: -19.30415, std: 7.51230, params: {'max_depth': 5},\n",
       " mean: -17.16534, std: 8.32303, params: {'max_depth': 6}]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 結果の確認 (valの対する結果)\n",
    "model_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最も結果が良かったハイパーパラメータ\n",
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最も結果が良かったハイパーパラメータの値を設定したモデル\n",
    "model = model_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8065343207878718"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test_s, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はもともとオーバーフィッティングしていなかったため，ハイパーパラメータの調整によって改善することはありませんでしたが，もちろんオーバーフィッティングしているケースには有効な施策であるため，覚えておきましょう．\n",
    "\n",
    "またランダムフォレストを含めた決定木系の手法の大きなメリットとして，各入力変数がどの程度重要であるかを定量評価した値が得られます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03498355, 0.00122218, 0.00721576, 0.00092089, 0.00898805,\n",
       "       0.39898526, 0.01894469, 0.04598164, 0.00571163, 0.01607173,\n",
       "       0.01889314, 0.00701622, 0.43506524])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各入力変数の重要度\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重要度の総和が1になっており，この値を使って考察したり説明できるため，実務でよく見るポイントの一つです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分類 (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer['data']\n",
    "t = breast_cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練と検証データの分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルのインスタンス化，学習\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9949748743718593"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415204678362573"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# スケーリング後のデータを使って学習\n",
    "model.fit(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test_s, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調整を行うハイパーパラメータの値の候補\n",
    "param_grid = [\n",
    "    {'max_depth': [1, 2, 3, 4, 5, 6]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交差検証法を使用したハイパーパラメータの各組合せでの学習\n",
    "model_cv = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\n",
    "model_cv.fit(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryosu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.91206, std: 0.01259, params: {'max_depth': 1},\n",
       " mean: 0.93467, std: 0.01763, params: {'max_depth': 2},\n",
       " mean: 0.93467, std: 0.01265, params: {'max_depth': 3},\n",
       " mean: 0.95226, std: 0.01543, params: {'max_depth': 4},\n",
       " mean: 0.93970, std: 0.02119, params: {'max_depth': 5},\n",
       " mean: 0.93970, std: 0.01632, params: {'max_depth': 6}]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 結果の確認 (valの対する結果)\n",
    "model_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最も結果が良かったハイパーパラメータ\n",
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最も結果が良かったハイパーパラメータの値を設定したモデル\n",
    "model = model_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9590643274853801"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test_s, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.55728165e-03, 4.80674523e-03, 7.04751333e-02, 6.67175039e-02,\n",
       "       0.00000000e+00, 6.38391868e-03, 1.44783793e-01, 2.56851226e-02,\n",
       "       1.01592678e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       7.17633429e-03, 3.57428435e-02, 5.48903143e-05, 7.63635594e-03,\n",
       "       8.65354259e-03, 3.50041088e-03, 4.29368206e-03, 4.42019691e-03,\n",
       "       8.32478506e-02, 1.93259510e-02, 1.79759936e-01, 8.96864960e-02,\n",
       "       4.76915506e-03, 2.20744844e-02, 6.78042092e-02, 1.24987366e-01,\n",
       "       4.10480850e-03, 8.33606129e-03])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各入力変数の重要度\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ロジスティック回帰\n",
    "\n",
    "シンプルであるが良く使われる手法のひとつです．回帰という名前がついているが，問題設定としては分類に使用する点に注意しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer['data']\n",
    "t = breast_cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練と検証データの分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルのインスタンス化，学習\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.957286432160804"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# スケーリング後のデータを使って学習\n",
    "model.fit(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9899497487437185"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（訓練データ）\n",
    "model.score(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766081871345029"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test_s, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調整を行うハイパーパラメータの値の候補\n",
    "param_grid = [\n",
    "    {'C': [0.01, 0.1, 1, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.01, 0.1, 1, 10]}], pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交差検証法を使用したハイパーパラメータの各組合せでの学習\n",
    "model_cv = GridSearchCV(LogisticRegression(), param_grid, cv=3)\n",
    "model_cv.fit(X_train_s, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryosu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.96985, std: 0.01223, params: {'C': 0.01},\n",
       " mean: 0.97990, std: 0.00935, params: {'C': 0.1},\n",
       " mean: 0.98492, std: 0.01624, params: {'C': 1},\n",
       " mean: 0.97236, std: 0.02323, params: {'C': 10}]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 結果の確認 (valの対する結果)\n",
    "model_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最も結果が良かったハイパーパラメータ\n",
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最も結果が良かったハイパーパラメータの値を設定したモデル\n",
    "model = model_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766081871345029"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証（検証データ）\n",
    "model.score(X_test_s, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロジスティック回帰の特徴は推論の時に出てくるため，違いについて紹介します．\n",
    "\n",
    "これまでの分類の手法であれば，新しいサンプルが得られた際の予測値は0か1かのカテゴリの値が得られます．Scikit-learnでは推論には`predict`を使用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データの一番最初のサンプルで試しに推論\n",
    "x_pred = [X_train_s[0]]\n",
    "y = model.predict(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この結果はどの手法でも同じですが，ロジスティック回帰を含めた**識別モデル**系の手法では，各カテゴリに属する確率まで求めることができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict_proba(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00160119 0.99839881]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果からわかる通り，各カテゴリの確率の総和が1となっており，確率が大きいほうのカテゴリ1が選ばれたことがわかります．異常か異常でないかといった分類の場合，異常or異常でないだけでなく，どのくらい異常そうであるかの確率までわかることで，閾値を設けやすくなります．この特性は次の章で紹介するニューラルネットワークでも同じです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means\n",
    "\n",
    "最後は教師なし学習である**クラスタリング**の手法として有名なk-meansです．分類では教師データとしてどのカテゴリに属しているかがわかっていたが，クラスタリングではその教師データがない状況で学習を行います．距離的に近いものをまとめるといった特性を持っています．\n",
    "\n",
    "例題では2つのクラスターをあらかじめ用意しておき，正しく分けられるかを確認していきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.randn(50, 2) - 3\n",
    "X2 = np.random.randn(50, 2) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結合\n",
    "X = np.r_[X1, X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x28640957438>"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは，k-meansを用いてクラスタリングを行いましょう．クラスタリングでは分けるクラスターの数がハイパーパラメータとして必要であることが一般的であり，`n_clusters`で指定します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KMeans(n_clusters=2)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデルをもとにクラスタリングを行いましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpyをうまく使うと，条件に当てはまるサンプルだけを抽出できるため，この機能を使って結果を可視化してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = X[y==0]\n",
    "X1 = X[y==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x28640b6c8d0>"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X0[:, 0], X0[:, 1], color='red')\n",
    "plt.scatter(X1[:, 0], X1[:,1 ], color='blue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

