{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNA Sequence Data Analysis",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hillbig/medical-ai-course-materials/blob/master/notebooks/DNA_Sequence_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yoiTUC_zXAb9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[![colab-logo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/japan-medical-ai/medical-ai-course-materials/blob/master/notebooks/Basenji.ipynb)\n",
        "\n",
        "# 実践編：ディープラーニングを使った配列解析\n",
        "\n",
        "近年，次世代シーケンサ（NGS; Next Generation Sequencer）の発展により，遺伝子の塩基配列を高速，大量，安価に読み取ることができるようになってきました．\n",
        "\n",
        "ここではディープラーニングを用いて，DNA配列からエピジェネティックな影響や転写制御を予測する問題に取り組みます．この予測モデルを使うことで，ある遺伝子変異が遺伝子発現にどのような影響を与えるのかを予測することができるようになります．\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CkqRZHc8crS4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 環境\n",
        "\n",
        "ここで用いるライブラリは\n",
        "\n",
        "\n",
        "*  Chainer\n",
        "*  Cupy\n",
        "*  matplotlib\n",
        "\n",
        "です．Google Colab上では，以下のようにしてインストールすることができます．以下のセルを実行（Shit+Enter）してください．\n"
      ]
    },
    {
      "metadata": {
        "id": "muhQoVk7c9y1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!set -ex\n",
        "!apt -y -q install cuda-libraries-dev-9-2\n",
        "!pip install cupy-cuda92==6.0.0a1\n",
        "!pip install chainer==6.0.0a1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mEPaj4MfdEyh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "インストールが完了したら，以下のセルを実行して，各ライブラリのバージョンを確認してください．\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "p4X-dmKrdDhd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "import cupy\n",
        "import matplotlib\n",
        "\n",
        "chainer.print_runtime_info()\n",
        "print('matplotlib:', matplotlib.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0nsNNeFEkjB_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Platform: Linux-4.14.65+-x86_64-with-Ubuntu-18.04-bionic\n",
        "Chainer: 6.0.0a1\n",
        "NumPy: 1.14.6\n",
        "CuPy:\n",
        "  CuPy Version          : 6.0.0a1\n",
        "  CUDA Root             : /usr/local/cuda\n",
        "  CUDA Build Version    : 9020\n",
        "  CUDA Driver Version   : 9020\n",
        "  CUDA Runtime Version  : 9020\n",
        "  cuDNN Build Version   : 7201\n",
        "  cuDNN Version         : 7201\n",
        "  NCCL Build Version    : 2213\n",
        "iDeep: Not Available\n",
        "('matplotlib:', '2.1.2')\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "MzdDwd7aeYmT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 配列解析について\n",
        "\n",
        "次世代シーケンサの発展・普及とともに，大量の遺伝子配列が読み取られるようになりました．そうした中で，塩基配列で表現された遺伝子型と病気や形態などの表現型との関係を推定するようなGWAS（Genome Wide Association Study; ゲノムワイド関連解析）がされてきましたが，遺伝子変異だけでは全ての表現型の変化を説明できないことがわかってきています．特に，非翻訳領域が遺伝子発現に影響を与え，表現型の変化を生じさせていることが様々な実験結果からわかってきています．遺伝子発現時に周辺領域がどのように影響を与えているのかを調べるために様々な手法が提案されています．（以下図）\n",
        "\n",
        "![代替テキスト](https://www.encodeproject.org/images/c45f4d8c-0340-4fcb-abe3-e4ff0bb919be/@@download/attachment/EncodeDatatypes2013-7.png)\n",
        "[Encode Projectより引用]\n",
        "\n",
        "例えば，ChIP-Seq（クロマチン免疫沈降）は，転写調節因子やそのほかのタンパク質が直接の相互作用を起こすDNAの特定部位を分離し，それらをシーケンシングして同定し，どの程度出現していたかを定量化します．これにより，タンパク質のDNA中の結合部位を正確かつ効率的に同定することができます．\n",
        "\n",
        "このような技術で抽出された配列を学習データとして利用し，DNA配列のみからそこが結合部位かどうかだけでなく，どの程度，出現していたのかというカバレッチ値を推定することで，DNA配列のみから，転写因子，クロマチンアクセシビリティ，ヒストン修飾を予測することができるようになり，様々な遺伝子変異に対する有益な洞察を与えてくれます．\n",
        "\n",
        "一方で，DNA配列中のどの領域がそのような特徴を持つのかを調べるためには非常に遠距離のDNA配列もいる必要があり，これが機械学習による解析を困難としていました．今回紹介する手法はこのような遠距離の関係を捉えるため，10万超の長さのDNA配列を入力として受け取り，128 bpごとにその領域がどの程度各手法で発現していたのか，カバレッジ値を予測するタスクを考えます．"
      ]
    },
    {
      "metadata": {
        "id": "db3ngEYHgSmd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## データセット\n",
        "\n",
        "ここでは，データセットとして[FANTOM5](http://fantom.gsc.riken.jp/5/)のCAGEデータセットを利用します．ここでは前処理が既に終わって配列と，各位置ごとの推定カバレッジ値が記録されたデータを利用します．下のセルを実行してデータをダウンロードしてください．\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LjxWi_2chwkX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/japan-medical-ai/medical-ai-course-materials/releases/download/v0.1/seq.h5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9yuEHGl_XC2B",
        "colab_type": "code",
        "outputId": "ba0b8763-37a3-4f07-9a82-b9eea126d3e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 562M\n",
            "drwxr-xr-x 2 root root 4.0K Nov 19 21:53 sample_data\n",
            "-rw-r--r-- 1 root root 562M Nov 13 09:51 seq.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l8NsMudgiHBI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "data.h5というファイルが正しくダウンロードされているかを確認してください．サイズは562MBです．\n",
        "\n",
        "data.h5はHDF5形式でデータを格納したファイルです．HDF5ファイルは，ファイルシステムと同様に，階層的にデータを格納することができ，行列やテンソルデータをそれぞれの位置で名前付きで格納することができます．\n",
        "\n",
        "HDF5形式のファイルを操作するためにh5pyというライブラリがあります．h5pyのFile()でファイルを開き，keys()というAPIでその中に含まれているキーを列挙します．取得したキーを使って格納されている各データを参照することができます．\n",
        "\n",
        "テンソルデータはnumpyと同様にshapeという属性でそのサイズを取得することができます．\n",
        "\n",
        "以下のセルを実行して格納されているデータを確認してください．"
      ]
    },
    {
      "metadata": {
        "id": "bBQVPyKxi-uE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "with h5py.File('seq.h5', 'r') as hf:\n",
        "    for key in hf.keys():\n",
        "    print(key, hf[key].shape, hf[key].dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T7OkqzE-jXlq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "(u'pool_width', ())\n",
        "(u'target_ids', (3,))\n",
        "(u'target_labels', (3,))\n",
        "(u'target_strands', (3,))\n",
        "(u'test_in', (444, 131072, 4))\n",
        "(u'test_out', (444, 1024, 3))\n",
        "(u'test_out_full', (444, 1024, 3))\n",
        "(u'train_in', (6240, 131072, 4))\n",
        "(u'train_out', (6240, 1024, 3))\n",
        "(u'valid_in', (338, 131072, 4))\n",
        "(u'valid_out', (338, 1024, 3))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NkAYTB2ZkEXr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "h5py形式のファイルをnumpyデータとして扱うには，コピーする必要があります．以下のコードは'train_in'というキーに対応するテンソルデータをnumpyデータとして読み出し，そのデータを一部を表示します．\n",
        "\n",
        "試しに最初のデータを取り出して，それの出力の値を表示してみます．"
      ]
    },
    {
      "metadata": {
        "id": "lik6qHPD4m9V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with h5py.File('seq.h5') as hf:\n",
        "    y = hf['train_out'][:100]\n",
        "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "    fig_size[0] = 20\n",
        "    fig_size[1] = 10\n",
        "    for i in range(3):\n",
        "        plt.bar(range(y.shape[1]), y[0,:,i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9dB69Y47k-y_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dilated Convolutionを用いた解析\n",
        "\n",
        "### 配列解析の戦略\n",
        "\n",
        "配列データを扱うためには大きく３つの戦略があります．\n",
        "\n",
        "一つ目は，配列中の順序情報は捨てて，配列をその特徴の集合とみなすことです．これはBag of Words（BoW）表現とよびます．このBoW表現は特徴に十分情報が含まれていれば強力な手法ですがDNA配列のような4種類の文字からなる配列やその部分配列だけではその特徴を捉えることは困難です．\n",
        "\n",
        "二つ目は配列中の要素を左から右に順に読み込んでいき計算していく手法です．これはRNNを用いて解析します．このRNNの問題点はその計算が逐次的であり計算量が配列長に比例するという点です．現在の計算機は計算を並列化することで高速化を達成していますがRNNは計算を並列化することが困難です．もう一つの問題は遠距離間の関係を捉えることが難しいという点です．RNNはその計算方式から，計算の途中結果を全て状態ベクトルに格納する必要があります．遠距離間の関係を捉えようとすると，多くの情報を覚えておかなければなりませんが状態ベクトルサイズは有限なので，多くの情報を忘れなければなりません．\n",
        "\n",
        "三つ目は配列データを1次元の画像とみなし，画像処理の時と同様にCNNを用いて解析する手法です．CNNはRNNの場合と違って各位置の処理を独立に実行できるため並列に処理することができます．また，後述するDilated Convolutionを使うことで各位置の処理は遠距離にある情報を直接読み取ることができます．次の章でDilated Convolutionについてさらに詳しくみていきます．\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RWl4yYm_nqg7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dilated Convolution\n",
        "\n",
        "従来の畳み込み層を使って配列解析をする場合を考えてみます．\n",
        "以下の図のようにある位置の入力の情報は各層で隣接する位置からしか読み込まれません．どのくらい離れた位置から情報を取得するかはカーネルサイズによって決定され，カーネルサイズがKの時，Dだけ離れた距離にある情報を取得するためにはD/K層必要となります．今回の問題の場合Dは数百から数万，Kは3や5といった値ですので必要な層数も百から万といった数になってしまい現実的ではありません．"
      ]
    },
    {
      "metadata": {
        "id": "gJ2MdbaHneLk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![orig conv](http://musyoku.github.io/images/post/2016-09-17/naive_conv.png)\n",
        "\n",
        "[WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)より引用"
      ]
    },
    {
      "metadata": {
        "id": "Gazys1FUoV4m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "それに対し，Dilated Convolution（atrous convolutionやconvolution weith holesともよばれます）は読み取る場所をずらしたところからうけとります．例えばDilation=4の場合，4だけ離れた位置から情報を受け取ります．このDilatedを倍々にしていき，カーネルサイズを2とした場合，Dだけ離れた位置の情報を受取るには log_2 D層だけ必要になります．今回のDが数百から数万の場合，10から20層程度あれば済むことになります．\n",
        "\n",
        "今回はこのDilated Convolutionを使うことで遠距離にある情報を考慮できるモデルを作成します．"
      ]
    },
    {
      "metadata": {
        "id": "Vl5f4eonQGU9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![dilated convolution](https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif)\n",
        "\n",
        "[WAVENET: A GENERATIVE MODEL FOR RAW AUDIO, blog](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)より\n"
      ]
    },
    {
      "metadata": {
        "id": "p0bcCznko_wD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ブロック\n",
        "\n",
        "それでは最初に，ネットワークの全体を設計します．\n",
        "このネットワークは二つのブロックから構成されます．\n",
        "\n",
        "１つ目のブロックは長さが2^17から配列を長さが2^10のベクトル列まで圧縮し，128 bpあたり1つのベクトルが対応する部分に対応し、SqueezeBlockが担当します．\n",
        "\n",
        "二つ目のブロックは遠距離にある情報を考慮して各ベクトルの値を計算していく部分であり、DilatedBlockが担当します。\n",
        "\n",
        "それでは・以下のコードを実行してみましょう．\n"
      ]
    },
    {
      "metadata": {
        "id": "5M6BDmVdpLkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "import cupy as cp\n",
        "\n",
        "bc = 16 # base channel\n",
        "\n",
        "default_squeeze_params = [\n",
        "    # out_ch, kernel, pool\n",
        "    [bc, 21, 2], #1 128 -> 64\n",
        "    [bc*2, 7, 4], #2  64 -> 16\n",
        "    [bc*4, 7, 4], #3  16 -> 4\n",
        "  [bc*8, 7, 4], #4  4 -> 1\n",
        "]\n",
        "\n",
        "\n",
        "default_dilated_params = [\n",
        "# out_ch, kernel, dilated\n",
        "  [bc*8, 3, 1],\n",
        "  [bc*8, 3, 2], \n",
        "  [bc*8, 3, 4], \n",
        "  [bc*8, 3, 8], \n",
        "  [bc*8, 3, 16], \n",
        "  [bc*8, 3, 32],\n",
        "  [bc*8, 3, 64]\n",
        "]\n",
        "\n",
        "\n",
        "class Net(chainer.Chain):\n",
        "\n",
        "    def __init__(self, squeeze_params=default_squeeze_params, dilated_params=default_dilated_params, n_targets=3):\n",
        "        super(Net, self).__init__()\n",
        "        self._n_squeeze = len(squeeze_params)\n",
        "        self._n_dilated = len(dilated_params)\n",
        "        with self.init_scope():\n",
        "            in_ch = 4\n",
        "            for i, param in enumerate(squeeze_params):\n",
        "                out_ch, kernel, pool = param\n",
        "                setattr(self, \"s_{}\".format(i), SqueezeBlock(in_ch, out_ch, kernel, pool))\n",
        "                in_ch = out_ch\n",
        "            for i, param in enumerate(dilated_params):\n",
        "                out_ch, kernel, dilated = param\n",
        "                setattr(self, \"d_{}\".format(i), DilatedBlock(out_ch, kernel, dilated))\n",
        "            self.l = L.ConvolutionND(1, None, n_targets, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x : (B, X, 4)\n",
        "        xp = cp.get_array_module(x)\n",
        "        h = xp.transpose(x, (0, 2, 1))\n",
        "        h = h.astype(xp.float32)\n",
        "                \n",
        "        for i in range(self._n_squeeze):\n",
        "            h = F.forget(self[\"s_{}\".format(i)], h)\n",
        "    \n",
        "        for i in range(self._n_dilated):\n",
        "            h = F.forget(self[\"d_{}\".format(i)], h)\n",
        "\n",
        "        h = self.l(h)\n",
        "        h = xp.transpose(h, (0, 2, 1))\n",
        "        return h\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kc3RNwK_qHzS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "このネットワークは初期化時の引数としてRootBlockに関するパラメータと，DilatedBlockに関するパラメータを受け取ります．\n",
        "\n",
        "それぞれ，出力チャンネル，カーネルサイズ，プーリングの三つ組からなるリストと，出力チャンネル，カーネルサイズ，dilatedサイズの三つ組からなるリストを受け取ります．"
      ]
    },
    {
      "metadata": {
        "id": "s3T5pRubrlba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "次に，ブロックの定義をします．"
      ]
    },
    {
      "metadata": {
        "id": "shOuWcBkrpOE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "import cupy as cp\n",
        "\n",
        "class WNConvolutionND(L.ConvolutionND):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(WNConvolutionND, self).__init__(*args, **kwargs)\n",
        "        self.add_param('g', self.W.data.shape[0])\n",
        "        norm = np.linalg.norm(self.W.data.reshape(\n",
        "            self.W.data.shape[0], -1), axis=1)\n",
        "        self.g.data[...] = norm\n",
        "\n",
        "    def __call__(self, x):\n",
        "        norm = F.batch_l2_norm_squared(self.W) ** 0.5\n",
        "        channel_size = self.W.data.shape[0]\n",
        "        norm_broadcasted = F.broadcast_to(\n",
        "            F.reshape(norm, (channel_size, 1, 1)), self.W.data.shape)\n",
        "        g_broadcasted = F.broadcast_to(\n",
        "            F.reshape(norm, (channel_size, 1, 1)), self.W.data.shape)\n",
        "        return F.convolution_nd(\n",
        "            x, g_broadcasted * self.W / norm_broadcasted, self.b, self.stride,\n",
        "            self.pad, self.cover_all, self.dilate)\n",
        "\n",
        "class SqueezeBlock(chainer.Chain):  \n",
        "    def __init__(self, in_ch, out_ch, kernel, pool):\n",
        "        super(SqueezeBlock, self).__init__()\n",
        "        \n",
        "        self.pool = pool\n",
        "        with self.init_scope():\n",
        "            pad = kernel // 2\n",
        "            self.conv = WNConvolutionND(1, in_ch, out_ch*2, kernel, pad=pad, stride=pool)\n",
        "      \n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h, g = F.split_axis(h, 2, 1)\n",
        "        h = h * F.sigmoid(g)\n",
        "        return h\n",
        "\n",
        "class DilatedBlock(chainer.Chain):\n",
        "     def __init__(self, out_ch, kernel, dilate):\n",
        "        super(DilatedBlock, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.conv = WNConvolutionND(1, out_ch, out_ch*2, kernel, pad=dilate, dilate=dilate)\n",
        "            self.conv1x1 = WNConvolutionND(1, out_ch, out_ch, 1)\n",
        "      \n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h, g = F.split_axis(h, 2, 1)\n",
        "        h = h * F.sigmoid(g)\n",
        "        h = self.conv1x1(h)\n",
        "        return h + x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrTAARyW2AYQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "WeightNormalizationはパラメータの表現を長さと向きに分解して表現する手法で，今回の系列問題のような場合に使われる正規化法です．\n",
        "\n",
        "SqueezeBlockは配列を縮めていき，長さ131072の配列を1024に縮めるためのブロックです．\n",
        "1次元配列を扱うためConvolutionNDを使い，最初の引数で1次元配列であることを指定しています．各層は畳み込み層，BatchNormalization層，MaxPooling層，Reluから構成されます．\n",
        "\n",
        "DilatedBlockはすでに長さ1024の長さになった配列に対し，Dilated Convolutionを使って遠距離にある情報も使って計算していく部分です．引数としてdilatedを受け取ります．Dilated Convolutionを使う場合は通常のConvolution層（今回はConvolutionNDだが，Convolution2Dも同様）の引数にdilatedを加えるだけで計算できます．\n",
        "\n",
        "また，計算の際は入力結果に現在の結果を足しこみます．これはResNetと呼ばれるネットワークで提案された手法です．これにより，学習がしやすくなります．\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "y-ZdRuhSq2Rq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "それでは，試しにネットワークを構築して，そこにサンプルデータを流してみましょう．\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DARrKIMurGiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "n = Net()\n",
        "size = 131072 # 128 * 1024\n",
        "batchsize = 4\n",
        "x = np.empty((batchsize, size, 4), dtype=np.bool)\n",
        "y = n.forward(x)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ydR6gwYCsATQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```\n",
        "(4, 1024, 3)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OyJ8lu_psGlk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ここで，もともとB= 4, L=131072, C=4だった配列が計算後はB=4, L=1024, C=3の配列となりました．"
      ]
    },
    {
      "metadata": {
        "id": "vLNgEVh0vjOt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "今回の学習では対数ポアソン損失関数を利用します．これはモデルはポアソン分布の唯一のパラメータである平均を出力し，そのポアソン分布を学習データを使った最尤推定をします．この際，学習対象パラメータ以外は無視しています．\n",
        "また，この最適化関数の最小値はそのままだと$0$にはならので，最小値である$t \\log t$をひいておき，損失関数の最小値が$0$となるようにします．"
      ]
    },
    {
      "metadata": {
        "id": "rgQmu0Pgvh0P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer.functions as F\n",
        "import math\n",
        "import sklearn\n",
        "import numpy as np\n",
        "\n",
        "def log_poisson_loss(log_x, t):\n",
        "    #return F.mean(F.exp(log_x) - t * log_\n",
        "    loss =  F.mean(F.exp(log_x) - t * log_x) \n",
        "    t = chainer.cuda.to_cpu(t.astype(np.float32))  \n",
        "    offset = F.mean(cp.array(t - t * np.ma.log(t)))\n",
        "    return loss - offset\n",
        "\n",
        "\n",
        "def log_r2_score(log_x, t):\n",
        "    return F.r2_score(F.exp(log_x), t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "40kTUr3O2lu5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "また，学習率の調整にCosineSchedulerを使います．ニューラルネットワークの学習では，徐々に学習率を小さくしていくと，より汎化性能の高い解を見つけられることがわかっています．焼きなまし法のように最初は学習率を高くして極小解にはまらないようにし，後半は徐々に学習率を0に近づけていき収束させるというものです．\n",
        "CosineSchedulerはCosine関数の0度から90度までの変化のように学習率を変化させます．また学習は初期が不安定なので最初にn_warmup回，学習率を0から初期学習率まで線形に増やしていきます．\n"
      ]
    },
    {
      "metadata": {
        "id": "QvjM_C-z2o8m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from chainer import training\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class CosineScheduler(training.Extension):\n",
        "\n",
        "    def __init__(self, attr='lr', init_val=0.0001, n_decays=200, n_warmups=3, target=None, optimizer=None):\n",
        "        self._attr = attr\n",
        "        self._target = target\n",
        "        self._optimizer = optimizer\n",
        "        self._min_loss = None\n",
        "        self._last_value = None\n",
        "        self._init_val = init_val\n",
        "        self._n_decays = n_decays - n_warmups\n",
        "        self._decay_count = 0\n",
        "        self._n_warmups = n_warmups\n",
        "\n",
        "    def __call__(self, trainer):\n",
        "        updater = trainer.updater\n",
        "        optimizer = self._get_optimizer(trainer)\n",
        "        epoch = updater.epoch\n",
        "        if epoch < self._n_warmups:\n",
        "            value = self._init_val / (self._n_warmups + 1) * (epoch + 1)\n",
        "        else:\n",
        "            value = 0.5 * self._init_val * (1 + math.cos(math.pi * (epoch - self._n_warmups) / self._n_decays))\n",
        "        self._update_value(optimizer, value)\n",
        "\n",
        "\n",
        "    def _get_optimizer(self, trainer):\n",
        "        return self._optimizer or trainer.updater.get_optimizer('main')\n",
        "\n",
        "    def _update_value(self, optimizer, value):\n",
        "        setattr(optimizer, self._attr, value)\n",
        "        self._last_value = value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "56jZaaD82p-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "最後に学習中に訓練データに意味を変えない変化を加えるData Augmentationを適用します．これは画像において回転させたり，平行移動させたりする場合と同じです．"
      ]
    },
    {
      "metadata": {
        "id": "UX2NE83o274Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "import random\n",
        "\n",
        "class PreprocessedDataset(chainer.dataset.DatasetMixin):\n",
        "\n",
        "    def __init__(self, xs, ys, max_shift):\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "        self.max_shift = max_shift\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xs)\n",
        "\n",
        "    def get_example(self, i):\n",
        "        # It applies following preprocesses:\n",
        "        #     - Cropping\n",
        "        #     - Random flip\n",
        "\n",
        "        x = self.xs[i]\n",
        "        y = self.ys[i]\n",
        "\n",
        "        if random.randint(0, 1):\n",
        "            x = x[::-1, :]\n",
        "            y = y[::-1, :]\n",
        "\n",
        "        s = random.randint(-self.max_shift, self.max_shift)\n",
        "        x = np.roll(x, s, axis=0)\n",
        "        return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9RCVvBw0v9i-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "これで全部準備ができました．残りはchainerのtrainerを改造して学習するだけです．以下のコードを実行してください．\n",
        "\n",
        "元々のデータ全体では学習に時間がかかるので、データ/`ratio`分だけを学習、検証用データとして利用します。今回`ratio`は20に設定されています。\n",
        "30分程度で学習が完了します．"
      ]
    },
    {
      "metadata": {
        "id": "b1_e0bE7wB48",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "import numpy as np\n",
        "from chainer.training import extensions\n",
        "from chainer import training\n",
        "import h5py\n",
        "\n",
        "ml_h5 = h5py.File('seq.h5')\n",
        "print(list(ml_h5.keys()))\n",
        "\n",
        "train_x = ml_h5['train_in']\n",
        "train_y = ml_h5['train_out']\n",
        "\n",
        "valid_x = ml_h5['valid_in']\n",
        "valid_y = ml_h5['valid_out']\n",
        "\n",
        "test_x = ml_h5['test_in']\n",
        "test_y = ml_h5['test_out']\n",
        "\n",
        "ratio = 20\n",
        "train_x = train_x[:len(train_x)//ratio]\n",
        "train_y = train_y[:len(train_y)//ratio]\n",
        "valid_x = valid_x[:len(valid_x)//ratio]\n",
        "valid_y = valid_y[:len(valid_y)//ratio]\n",
        "\n",
        "\n",
        "max_shift_for_data_augmentation = 3\n",
        "train = PreprocessedDataset(train_x, train_y, max_shift_for_data_augmentation)\n",
        "val = chainer.datasets.TupleDataset(valid_x, valid_y)\n",
        "\n",
        "batchsize = 8\n",
        "\n",
        "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
        "val_iter = chainer.iterators.SerialIterator(val, batchsize, repeat=False, shuffle=False)\n",
        "\n",
        "model = L.Classifier(Net(), lossfun=log_poisson_loss, accfun=log_r2_score)\n",
        "\n",
        "lr = 0.002\n",
        "optimizer = chainer.optimizers.Adam(alpha=lr, beta1=0.97, beta2=0.98)\n",
        "optimizer.setup(model)\n",
        "optimizer.add_hook(chainer.optimizer_hooks.GradientClipping(threshold=0.01))\n",
        "\n",
        "\n",
        "updater = training.updaters.StandardUpdater(\n",
        "     train_iter, optimizer, device=0)\n",
        "\n",
        "n_epochs = 60\n",
        "n_warmups = 5\n",
        "out = \"out\"\n",
        "trainer = training.Trainer(updater, (n_epochs, 'epoch'), out=out)\n",
        "trainer.extend(CosineScheduler(attr='alpha', init_val=lr, n_decays=n_epochs, n_warmups=n_warmups), trigger=(1, 'epoch'))\n",
        "\n",
        "trainer.extend(extensions.Evaluator(val_iter, model, device = 0))\n",
        "trainer.extend(extensions.LogReport())\n",
        "trainer.extend(extensions.snapshot_object(model, 'model_epoch_{.updater.epoch}'), trigger=(1, 'epoch'))\n",
        "\n",
        "trainer.extend(extensions.PrintReport(\n",
        "          ['epoch', 'main/loss', 'validation/main/loss', 'elapsed_time']), trigger = (0.1, 'epoch'))\n",
        "\n",
        "# trainer.extend(extensions.ProgressBar())\n",
        "           \n",
        "trainer.run()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mN4nDWQW3Dki",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "学習は成功したならば，実際にモデルが出力されているのかを確認しましょう．"
      ]
    },
    {
      "metadata": {
        "id": "hfT1yyTl3C9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -l out/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4eQCDXG3L6e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "次に，テストデータに対して学習データを適用します．学習が終わったモデルを読み込み，テストデータに対してモデルを適用します．"
      ]
    },
    {
      "metadata": {
        "id": "UfJ7ZEQX3UQS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "import chainer.links as L\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_n_epoch = 10\n",
        "out_dir = 'out'\n",
        "model = L.Classifier(Net())\n",
        "chainer.serializers.load_npz('{}/model_epoch_{}'.format(out_dir, model_n_epoch), model)\n",
        "predictor = model.predictor\n",
        "\n",
        "print(len(test_x))\n",
        "with chainer.no_backprop_mode():\n",
        "    test_y_estimated = F.exp(predictor(test_x[:1]))\n",
        "\n",
        "test_y = test_y[:1]\n",
        "\n",
        "print(test_y_estimated.shape)     \n",
        "print(test_y_estimated[0,:,0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dlA0DLxY3atL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "結果を抜粋して表示してみましょう．ここでは3つ目（i=2）の出力について正解と推定結果を出力しています．今回の場合，学習データが少なく，学習回数も少ないためそれほど精度がでていません．"
      ]
    },
    {
      "metadata": {
        "id": "nN4rkeuU7rjV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = test_y_estimated.data\n",
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 20\n",
        "fig_size[1] = 10\n",
        "i = 2\n",
        "plt.bar(range(y.shape[1]), y[0,:,i])\n",
        "plt.bar(range(y.shape[1]), test_y[0,:,i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pmaz68ZrsFOb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "時間に余裕があれば学習のn_epochsを60から200程度に増やしたり，総数を増やしたり，チャンネル数を増やしたり，学習データ数（ratio=20をratio=5などにして）に増やしたりして，より高精度なモデルが学習できるのかを調べてみましょう．\n"
      ]
    }
  ]
}