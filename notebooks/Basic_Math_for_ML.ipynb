{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "[![colab-logo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/japan-medical-ai/medical-ai-course-materials/blob/master/notebooks/Basic_Math_for_ML.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4_DY-zYo0KPz"
      },
      "source": [
        "# 機械学習に必要な数学の基礎\n",
        "\n",
        "この章では，ディープラーニングを含めた機械学習に必要な数学の基礎，微分，線形代数，統計の3つについて，簡潔に紹介していきます．\n",
        "\n",
        "\n",
        "## 微分\n",
        "\n",
        "微分は機械学習においては**パラメータ更新（学習）**の際に重要な考え方です．なぜ重要なのかを説明した後に，その数式について理解していきます．\n",
        "\n",
        "### 微分はどこで使われているか\n",
        "\n",
        "「微分は何に使えるか」を考える前に，**微分は何が求まるのか**をおさらいします．微分は**接線の傾き**に対応します．以降，例も交えて具体的に見ていきます．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/01.png)\n",
        "\n",
        "この図の関数において，$a$ の点における接線の傾きというのは赤い直線の傾きを指し，例えば，傾きが$+3$のようになっています．傾きが$+3$ということは，入力の値が$1$増えたら，関数の値が$3$増えることを意味します．このように右肩上がりな直線の傾きは正の値になります．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/02.png)\n",
        "\n",
        "それに対し，この $b$ の点においては，接線の傾きは右肩下がりであるため，接線の傾きは負の値であり，傾き$-1$のようになっています．微分はこのように与えられた関数の各点の傾きを求めることができます．\n",
        "\n",
        "それでは，本題に戻りましょう．この各点の接線の傾きが求まると何に使えるのでしょうか．\n",
        "\n",
        "機械学習では，できるだけ真の値に近い良い予測値を得ることを目標とします．学習の際には入力とその望ましい出力$t$のペアからなる教師データ $(x, t)$ に対して，学習対象の関数の予測値 $y=f(x)$ がなるべく$t$に近い値になるように関数$f(x)$を学習することが期待されます．近い値というとその差分である ​$t-y$ が小さくなることが望ましいといえますが，厳密には，​$t-y$ を小さくしたいと考えれば，​$y\\rightarrow\\infty$ とすれば ​$t-y$ も小さくなりますが，そういう話ではなく，$t$ と $y$ の差を小さくしたいため，$|t-y|$ や $(t-y)^{2}$ が小さくなることが望ましいと考えられます．$t$と$y$の差を小さくしたいという目的であれば，$|t-y|$ を小さくするのでも良いのですが，$(t-y)^{2}$ を小さくする方が，数学的に取り扱いがしやすく，一般的にこちらが採用されます．この $(t-y)^2$ のことを**二乗誤差**と呼び，機械学習では，この二乗誤差のような目的関数の値を最も小さくできるように調整することがゴールとなります．\n",
        "\n",
        "それでは二乗誤差が最小となる点を求めたいと考えます．$(t-y)^2$の傾きは今の予測値をどのように変えればよいかという情報を与えてくれます．もし$(t-y)^2$の傾きが正であれば，今は関数の右肩上がりの領域にいるので，値を小さくすればよいとわかります．逆に傾きが負であれば，値を大きくすればよいとわかります．傾きが$0$であれば，誤差が最小だということがわかります．このように微分の傾きは最適化したい関数の様々な情報を与えてくれます．これが微分を学ぶ大きなモチベーションとなっています．目的関数を最大化したい，最小化したいという問題は普遍的な課題設定であり，これらの最適な点を求めるために活躍するツールが微分である．そう考えると，微分を学ぶモチベーションが大きく湧いたのではないでしょうか．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/03.png)\n",
        "\n",
        "### 2点間を通る直線の傾き\n",
        "\n",
        "それでは，微分の原理を理解していくために，まずは中学校で習った下図に示す2点間を通る直線の傾き $a$ を求めてみましょう．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/04.png)\n",
        "\n",
        "中学校の数学で習いましたが、「傾き$a$ = $f(x)$の変化量/$x$の変化量」より、中学校の時にならったこの考え方の通り，傾きは\n",
        "$$\n",
        "a = \\dfrac{f(x_{2}) - f(x_{1})}{x_{2}-x_{1}}\n",
        "$$\n",
        "と求まります．このように傾きは入力の単位量の変化に対し，関数の値がどれだけ変わるかを表します．この考え方で微分の大部分を説明できます．\n",
        "\n",
        "### 1点での接線の傾き\n",
        "\n",
        "それでは，中学校で習った内容から，さらに与えられた関数の接線の傾きまで求められるように考えていきましょう．そのためには，**極限**を知っておく必要があります．極限とは，$\\lim$ の下に書いた条件に値を近づけていく考え方です．例えば，\n",
        "$$\n",
        "\\displaystyle \\lim _{x\\rightarrow 0}3x=3\\times 0=0\n",
        "$$\n",
        "は，$x$という変数を$0$に近づけていったときに式の値がどのような値になるかを与えます．このような単純な式の場合は極限の考え方は，直接値を代入した場合（$x=0$など）と違いませんが，点の**動き**を表現したいときに有効な手段であり，この考えが微分でも登場します．\n",
        "\n",
        "それでは，次の問題として，下図のある点 $x$ における接線の傾き$a$を求めていきましょう．今回は2点ではない，ある1点での接線の傾きです．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/05.png)\n",
        "\n",
        "さて，ここで問題が生じます．接線の傾きを求めようにもここまでの知識では1点だけでは傾きを求めることができません．そこで，さきほど考えた2点を通る直線と極限を組み合わせて，接線を求められないか考えてみましょう．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/06.png)\n",
        "\n",
        "$x$ から $h$ だけ離れた点 $x+h$ を考え，この2点を通る直線の傾きを求めることとします．この場合，2点間を通る直線の傾きと変わらないのですが，$h \\rightarrow 0$ のようにすれば，理論上，開始点と終了点の2点が1点に収束し，1点での接線として考えることができ，直線の傾きは\n",
        "$$\n",
        "\\begin{aligned}\n",
        "a &=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{\\left( x+h\\right) -x}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{h}\\\\\n",
        " &=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{h}\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "となります．この式は一般的に微分という言葉で認識されていることが多いですが，厳密には**導関数**と呼び，$f'(x)$ で表されます．導関数を求めることを**微分する**といいます．また，記号の使い方として，\n",
        "$$\n",
        "(\\cdot)' = \\dfrac{d}{dx}(\\cdot)\n",
        "$$\n",
        "のように表しても構いません．この場合の$d$は差分（difference）を表しており，対象の値の変化量/$x$の変化量とみてもよいです．この記法はどの変数を対象に微分しているかが明確になるのが煩雑ですが正確な表現をすることができます．\n",
        "\n",
        "このように，中学校までに習った数学と極限の考え方を組み合わせるだけで微分の基本を理解することができました．\n",
        "\n",
        "### 微分の公式\n",
        "\n",
        "微分にはいくつか覚えておくと便利な公式があります．特に，機械学習を勉強し始める最初の段階では，以下の3つの公式だけで多くの関数を微分できます．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left( 1\\right) ^{'}&=0\\\\\n",
        "\\left( x\\right) ^{'}&=1\\\\\n",
        "\\left( x^{2}\\right) ^{'}&=2x\n",
        "\\end{aligned}\n",
        "$$\n",
        "これらの微分の公式をこれからよく使うため，暗記しておきましょう．これらは公式として覚えてしまえばよいのですが，ここでは勉強もかねてこの公式を導いてみましょう．数式に対して，あまり余裕がない段階であれば公式だけ覚えて読み飛ばしていただいても構いません．\n",
        "\n",
        "まずは，$f(x)=1$のときを考えましょう．この式の意味は，変数がどんな値であっても関数の値は$1$であるということから，$f(x)=1, f(x+h)=1$となります．さて，導関数の計算を行うと，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f'(x)&=\\left( 1\\right)'\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {1-1}{h} \\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {0}{h}\\\\\n",
        "&=\\lim _{x\\rightarrow 0}0\\\\\n",
        "&=0\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "が得られました．\n",
        "\n",
        "つぎに，$f(x)=x$ のときを考えてみましょう．$f(x+h) = x+h$ となり，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f'\\left( x\\right) &=\\left( x\\right)'\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {f\\left( x+h\\right) -f\\left( x\\right) }{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {\\left( x+h\\right) -x}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {h}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}1\\\\\n",
        "&=1\\end\n",
        "{aligned}\n",
        "$$\n",
        "\n",
        "が得られます．\n",
        "\n",
        "最後に，$f(x) = x^{2}$ の場合，$f(x+h) = (x+h)^{2}$ となります．よくある間違いとして，$f(x+h) = x^{2} + h$ や $f(x+h) = x^{2} + h^{2}$ のように考えてしまう人を見かけますが，$f(x) = (x)^{2}$ と考えると，$x$ の部分が $x+h$ に代わるため，$f(x+h) = (x+h)^{2}$ です．このとき，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f\\left( x\\right) &=\\left( x^{2}\\right)'\\\\\n",
        "&=\\lim_{h\\rightarrow 0}\\dfrac {f{\\left( x+h\\right) }-f\\left( x\\right) }{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {\\left( x+h\\right) ^{2}-x^{2}}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {\\left( x^{2}+2xh+h^{2}\\right) -x^{2}}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {2xh+h^{2}}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\dfrac {\\left( 2x+h\\right) h}{h}\\\\\n",
        "&=\\lim _{h\\rightarrow 0}\\ (2x + h)\\\\\n",
        "&=2x\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "が得られ，3つの公式を導くことができました．\n",
        "\n",
        "それでは，次の２つの例題を考えながら，具体的な微分の計算に慣れていきましょう．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "( 3x^{2})'&=3\\times (x^{2})'\\\\\n",
        "&=3\\times 2x\\\\\n",
        "&=6x\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "この $(3x^{2})' = 3 \\times (x^{2})'$ の部分に注目してください．微分では，このように定数の係数（変数にかかる数）は微分の演算の外側に出すことができます．また，次の例題でも新しい特性があります．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left( 3x^{2}+4\\right)'&=\\left( 3x^{2}\\right)'+\\left( 4\\right)'\\\\\n",
        "&=3\\times \\left( x^{2}\\right)'+4\\times \\left( 1\\right)'\\\\\n",
        "&=3\\times 2x+4\\times 0\\\\\n",
        "&=6x\n",
        "\\end{aligned}\n",
        "$$\n",
        "この例題では，$\\left( 3x^{2}+4\\right)'=\\left( 3x^{2}\\right)'+\\left( 4\\right)'$ のように，全体の和をとった後に微分の演算を行っても，それぞれで微分の演算を行った後に和の計算をしても同じとなっています．これは微分の**線形性**と呼ばれる性質であり，この性質を使うことで，微分を簡単に計算できるようになります．\n",
        "\n",
        "### 合成関数の微分\n",
        "\n",
        "機械学習のアルゴリズムで登場する関数は複雑なものも多く，それらの微分の計算も複雑になりがちです．例えば，\n",
        "$$\n",
        "\\left\\{ (3x + 4)^{2} \\right\\}'\n",
        "$$\n",
        "\n",
        "の場合，$3x+4$ という内側の部分と $(\\cdot)^{2}$ という外側の部分で構成されています．この式を$(9x^2 + 24x + 16)'$のように展開してから微分を計算してもよいのですが，これが3乗や4乗となってくると展開するのも大変になります．この時に役に立つ考え方が**合成関数の微分**です．\n",
        "\n",
        "まず内側の関数を $u = (3x+4)$ とおいて，\n",
        "$$\n",
        "\\left\\{ (3x + 4)^{2} \\right\\}' = (u^{2})'\n",
        "$$\n",
        "\n",
        "とします．ここで，$(\\cdot)'$ をもう少し厳密に考える必要が出てきます．今は，$x$ と $u$ の2つの変数が登場しており，$(\\cdot)'$ では，$x$ で微分しているのか $u$ で微分しているのかの区別がつきません．そこで，多少複雑に見えますが，先程紹介した$d$を使った記法で微分する変数を厳密に記述すると，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\left\\{ (3x + 4)^{2} \\right\\}' &= \\dfrac{d}{dx} \\left\\{ (3x + 4)^{2} \\right\\} \\\\\n",
        "&= \\dfrac{d}{dx} (u^2) \\\\\n",
        "&=  \\dfrac{d}{dx} f(u) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "となり，$u$ の関数 $f(u) = u^{2}$ に対して，$x$ で微分していることがわかります．．$x$を変化させた時に$f(u)$がどれだけ変化するのかはわかりません．そこで，$x$を単位量変化させた時に$u$がどれだけ変化するのか，$u$を単位量変化させた時に$f(u)$がどれだけ変化するのかを考え，それらの積を考えます．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac{df(u)}{dx} = \\dfrac{du}{dx} \\dfrac{df(u)}{du}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "この時，$du$は分子と分母に出現しているので約分して消去してしまえば左辺と右辺は同じであるといった考え方を適用することができます．つまり，合成関数の計算は内側の微分と外側の微分をそれぞれ行い，その結果を掛け合わせれば良いわけです．それぞれの微分の計算は\n",
        "$$\n",
        "\\begin{aligned}\n",
        " \\dfrac{du}{dx} &= \\dfrac{d}{dx} (3x+4) = 3 \\\\\n",
        " \\dfrac{df(u)}{du} &= \\dfrac{d}{du} u^{2} = 2u \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "となります．これより，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac{df(u)}{dx} &= \\dfrac{du}{dx} \\dfrac{df(u)}{du} \\\\\n",
        " &= 3 \\times 2u \\\\\n",
        " &= 3 \\times 2(3x+4) \\\\\n",
        " &= 6(3x+4)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "が得られます．数式こそ多少複雑に見えますが，内側と外側をそれぞれ微分して掛け合わせるだけであり，実際の計算は慣れると簡単に行うことが出来ます．なお，今回は分数の約分という形で大雑把に説明しましたが，実際には極限を使って合成関数の微分が成り立つことを証明できます．ニューラルネットワークの学習では合成関数の微分を使用する場面が何度も登場するため，この計算方法をしっかりと覚えておきましょう．\n",
        "\n",
        "### 偏微分\n",
        "\n",
        "微分最後のトピックとして，**偏微分**を紹介します．偏微分は多変数関数の微分です．機械学習では，1つの入力変数 $x$ から出力変数 $y$ を予測するケースは稀であり，基本的には，複数の入力変数 $x_{1}$, $x_{2}$, $\\ldots$, $x_{M}$ を用いて出力変数 $y$ を予測します．例えば，家賃を予測する場合，部屋の広さだけで予測するよりも，駅からの距離や犯罪発生率などを考慮した方が予測の性能は高まりそうだと考えられます．多変数 $x_{1}$, $x_{2}$, $\\ldots$, $x_{M}$ を考慮した多変数関数 $f(x_{1}, x_{2}, \\ldots, x_{M})$ では，各変数で微分することを偏微分と呼び，\n",
        "$$\n",
        "\\dfrac{\\partial}{\\partial x_{m}} f(x_{1}, x_{2}, \\ldots, x_{M})\n",
        "$$\n",
        "\n",
        "のように表します．大雑把には，$d$ が $\\partial$ に変わっただけです．しかも，計算方法は至って単純であり，$\\dfrac{\\partial}{\\partial x_{m}}$ の場合は $x_{m}$ 以外は定数と考えて，$x_{m}$ のみ着目して微分を行います．\n",
        "\n",
        "例題で具体的な計算の流れを確認していきましょう．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}+4x_{2}\\right) &=\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}\\right) +\\dfrac {\\partial }{\\partial x_{1}}\\left( 4x_{2}\\right) \\\\\n",
        "&=3\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( x_{1}\\right) +4x_{2}\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( 1\\right) \\\\\n",
        "&=3\\times 1+4x_{2}\\times 0\\\\\n",
        "&= 3\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "偏微分でも微分と同じ線形性の性質が適用できます．今回のケースでは，$x_{1}$ にだけ着目するため，$x_{2}$ は定数として扱うことを把握しておけば上記の計算の流れが理解できるはずです．これが偏微分であり，参考書にはここからさらに全微分の話に入っていくことが多いですが，ひとまずここまでの計算の方法を理解しておけば，この後の計算は理解することができるはずです．\n",
        "\n",
        "## 線形代数\n",
        "\n",
        "### 線形代数は何に役立つのか\n",
        "\n",
        "次に重要な概念が**線形代数**です．ベクトル，行列，ランク，逆行列などの概念が登場します．\n",
        "\n",
        "さて，微分と同様に理論を学ぶ前に，これから学ぶ線形代数という学問がみなさんにとってどのようなメリットをもたらしてくれるかを考えましょう．$x_{1}$ や $x_{2}$ のように機械学習の中では，似たような複数の変数が登場してきます．これらの複数の変数間の関係を記述する上で線形代数は最も単純な数学ですが驚くほど多くの問題を扱うことができる重要な概念です．ぜひ身に着けていきましょう．\n",
        "\n",
        "### スカラー，ベクトル，行列，テンソル\n",
        "\n",
        "最初に線形代数で使われる**スカラー**，**ベクトル**，**行列**，**テンソル**の4つを解説します．\n",
        "\n",
        "スカラーは，1つの値もしくは変数のことです．例えば，\n",
        "$$\n",
        "x, \\ y,\\  M,\\  N\n",
        "$$\n",
        "のように表します．スカラーは例えば温度や身長といった単一の量を表すことに使われます．\n",
        "\n",
        "ベクトルは，複数のスカラーを縦方向（もしくは横方向）に集めて並べたものであり，\n",
        "$$\n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2} \\\\\n",
        "x_{3}\n",
        "\\end{bmatrix}, \\\n",
        "\\boldsymbol{y}=\\begin{bmatrix}\n",
        "y_{1} \\\\\n",
        "y_{2} \\\\\n",
        "\\vdots \\\\\n",
        "y_{N}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "ように表します．ベクトルの表記は太文字とする場合が多く，スカラーかベクトルかを一目で区別できるようにしています．ベクトルを縦方向に定義するか横方向に定義するかは業界によって異なっていますが，数学や機械学習では縦方向で定義している論文や参考書が多いため，本講義では**ベクトルは縦方向**で統一します．\n",
        "\n",
        "行列はさらに複数のベクトルをまとめたものであり，\n",
        "$$\n",
        "\\boldsymbol{X}=\\begin{bmatrix}\n",
        "x_{11} & x_{12} \\\\\n",
        "x_{21} & x_{22} \\\\\n",
        "x_{31} & x_{32}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "のように表します．行列のサイズは行と列で表現します．例えば，この $\\boldsymbol{X}$ は3行2列であり，サイズが(3, 2)の行列と言います．また，行列の値が実数の場合がほとんどであり，よく $\\boldsymbol{X} \\in \\mathcal{R}^{3 \\times 2}$のようにサイズを簡潔に表現することがあるため，こちらも覚えておきましょう．行は文字の中に横棒が２つ入っているため横方向の値の集まり，列は文字の中に縦棒が２つはいっているため縦方向の値というように覚えると楽かもしれません．行列の表記は大文字または大文字の太文字とする場合が多いです．\n",
        "\n",
        "最後に，テンソルは行列をさらにまとめたものであり，図のように行列を奥行き方向にさらに並べたものとみなすことができます．さらにテンソル同士を並べたものもテンソルとよばれます．例えば，RGB (Red Green Blue) などの色空間で表現するカラー画像を表現する場合，（行番号，列番号，色）の3つの軸で一つの値を指定することができ，テンソルで表現することができます．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/07.png)\n",
        "\n",
        "\n",
        "\n",
        "これまで紹介してきたように，値を並べたものは，スカラー $\\subset$ ベクトル $\\subset$ 行列 $\\subset$ テンソルのような関係がありました．線形代数では $\\boldsymbol{y}$ や $\\boldsymbol{X}$ といった文字だけで式変形をしていくため，どのような形の数値が取り扱われているかわかりくいのですが，これはベクトルなどと常に意識しておくことでその形を見失わないように注意しましょう．\n",
        "\n",
        "|        | 小文字         | 大文字         |\n",
        "| ------ | -------------- | -------------- |\n",
        "| 細文字 | スカラーの変数 | スカラーの定数 |\n",
        "| 太文字 | ベクトル       | 行列，テンソル |\n",
        "\n",
        "### 足し算・引き算\n",
        "\n",
        "行列やベクトルの演算について覚えていきましょう．足し算は同じサイズの行列，ベクトル間だけで成立し，\n",
        "$$\n",
        "\\begin{aligned}&\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "3\n",
        "\\end{bmatrix}+\\begin{bmatrix}\n",
        "4 \\\\\n",
        "5 \\\\\n",
        "6\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "1+4 \\\\\n",
        "2+5 \\\\\n",
        "3+6\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "7 \\\\\n",
        "8 \\\\\n",
        "9\n",
        "\\end{bmatrix}\\\\\n",
        "&\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6\n",
        "\\end{bmatrix}+\\begin{bmatrix}\n",
        "7 & 8 & 9 \\\\\n",
        "10 & 11 & 12\n",
        "\\end{bmatrix}=\\begin{bmatrix}\n",
        "8 & 10 & 12 \\\\\n",
        "14 & 16 & 18\n",
        "\\end{bmatrix}\\end{aligned}\n",
        "$$\n",
        "このように行列やベクトルの中の**要素**で対応する場所を足し合わせます．引き算も同様です．計算としては単純なものであり，特別なことはありません．**同じサイズでないと計算が成立しない**ということを覚えておきましょう．\n",
        "\n",
        "### 内積\n",
        "\n",
        "同じサイズのベクトル間では内積が定義できます．内積は同じ位置で対応する値を掛けていき，それらを足し合わせたものです．\n",
        "$$\n",
        "\\begin{aligned}&\\begin{bmatrix}1 \\\\2 \\\\3\\end{bmatrix}\\cdot \\begin{bmatrix}4 \\\\5 \\\\6\\end{bmatrix}= 1\\cdot4 + 2 \\cdot 5  + 3 \\cdot 6 = 36 \\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "### かけ算（行列積）\n",
        "\n",
        "行列の掛け算は複数パターンあり，一般的に掛け算として認識されているものは**行列積**と呼ばれます．それ以外には外積や要素積（アダマール積）などがあります．行列$A$と行列$B$の行列積は$A$の各行と$B$の各列の内積を並べたものとして定義されます．例えば行列Aの2行目の行ベクトルと行列Bの3列目の列ベクトルの内積は結果の行列Cの2行3列目に対応します．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/matrixproduct.png)\n",
        "\n",
        "そして，内積が定義される条件はベクトルのサイズが等しいということでしたが，ここでもそれが成り立つために，Aの行のサイズ（=Aの列数）とBの列のサイズ（=Bの行数）が一致する必要があります．以下のように，「行」「列」と線を引きながら計算していきましょう．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/08.png)\n",
        "\n",
        "このように，単純に足し算や引き算のように要素ごとの積を扱うわけではないため，最初は計算に慣れが必要です．そして，行列積では計算が成り立つためには，以下の条件を満たす必要があります．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/09.png)\n",
        "\n",
        "この行列積は線形代数や機械学習の多くの問題で使われます．行列積はベクトルの集まりとベクトルの集まりの間の大量の内積計算をするようなものです．また，行列では割り算に相当する演算はありませんが，後述する逆行列を使って$4 / 2 = 4 \\times \\dfrac{1}{2}$ のように割り算を逆数（逆行列）の掛け算として記述します．\n",
        "\n",
        "それでは，この計算条件の確認も踏まえて，下記の３つを練習問題として解いてください．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\left( 1\\right) \\begin{bmatrix} 1 & 2 \\end{bmatrix}\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\\\\ \n",
        "&\\left( 2\\right) \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix}\\\\ \n",
        "&\\left( 3\\right) \\begin{bmatrix} 1 & 2 \\end{bmatrix}\\begin{bmatrix} 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\\end{aligned}\n",
        "$$\n",
        "\n",
        "こちらが解答です．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\left( 1\\right) \\begin{bmatrix} 1 & 2 \\end{bmatrix}\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = 1\\times 3 + 2 \\times 4 = 11\\\\ \n",
        "&\\left( 2\\right) \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix}1\\times 5 + 2\\times 6 \\\\3 \\times 5 + 4 \\times 6\\end{bmatrix} = \\begin{bmatrix}17 \\\\ 39\\end{bmatrix}\\\\ \n",
        "&\\left( 3\\right) \n",
        "\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "3 & 4 \\\\\n",
        "5 & 6\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "3 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "3\\times 3+4\\times 1 \\\\\n",
        "5\\times 3 +6\\times 1\n",
        "\\end{bmatrix}\n",
        "& \\ \\ =\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "13 \\\\\n",
        "21\n",
        "\\end{bmatrix}\n",
        "=1 \\times 13+2\\times 21\n",
        "=55\n",
        "\\end{aligned}\n",
        "$$\n",
        "計算のイメージは沸いたでしょうか．実は，この３つの計算は機械学習においてよく出てくる形の計算です．押さえておくべきポイントとして，演算後に形が変わることを覚えておきましょう．\n",
        "\n",
        "### ベクトル，行列のサイズ\n",
        "\n",
        "計算を行う上でベクトル，行列の**サイズ**を常に意識しておくことが機械学習を円滑に理解するためのポイントとなります．例えば先ほどの３つの練習問題のサイズがどのように変化したかをまとめると，\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/10.png)\n",
        "\n",
        "のとなります．もともとがベクトルや行列であっても，演算後にスカラーになるケースもあることがわかります．なぜこのサイズが変化する感覚をつかんでおくことが大事になるのでしょうか．\n",
        "\n",
        "今回は計算結果を数値で追っていましたが，これからの計算はすべて数値ではなく文字で表現して扱います．例えば $\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{y}$ を見て，スカラーであると瞬時に判断できるようになる必要があります．ここで，ベクトルは縦向きで定義するため，横向きのベクトルは**転置**（記号は上付きの $T$）を使うことによって表現しています．\n",
        "\n",
        "### 転置\n",
        "\n",
        "ベクトルは縦向きが基本としていましたが，先程の計算問題などでは，しばしば横向きのベクトルが出てきました．そこで登場するベクトルの縦と横を入れ替える演算のことを**転置**（Transpose）と呼びます．例えば，\n",
        "$$\n",
        "\\begin{aligned}\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "3\n",
        "\\end{bmatrix}, \\ \n",
        "\\boldsymbol{x}^{T}=\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix} \\\\\n",
        "\\boldsymbol{X}&=\\begin{bmatrix}\n",
        "1 & 4 \\\\\n",
        "2 & 5 \\\\\n",
        "3 & 6\n",
        "\\end{bmatrix}, \\\n",
        "\\boldsymbol{X}^{T}=\\begin{bmatrix}\n",
        "1 & 2 & 3 \\\\\n",
        "4 & 5 & 6\n",
        "\\end{bmatrix}\\end{aligned}\n",
        "$$\n",
        "のようになります．このように転置自体の演算は簡単です．先程の行列積の演算も正確には各行の転値と行列の内積となります．転置では下記３つの公式を覚えておくと，これからの計算が楽になります．\n",
        "$$\n",
        "\\begin{aligned}&\\left( 1\\right) \\ \\left( \\boldsymbol{A}^{T}\\right)^{T}=\\boldsymbol{A}\\\\\n",
        "&\\left( 2\\right) \\ \\left( \\boldsymbol{A}\\boldsymbol{B}\\right) ^{T}=\\boldsymbol{B}^{T}\\boldsymbol{A}^{T}\\\\\n",
        "&\\left( 3\\right) \\ \\left( \\boldsymbol{A}\\boldsymbol{B}\\boldsymbol{C}\\right) ^{T}=\\boldsymbol{C}^{T}\\boldsymbol{B}^{T}\\boldsymbol{A}^{T}\\end{aligned}\n",
        "$$\n",
        "\n",
        "### 単位行列\n",
        "\n",
        "単位行列とは，スカラーの１に対応した性質をもつ行列です．どのような性質かというと，$10\\times1$ のように，その数を任意の数に乗じても変わらないという性質です．行列の演算において，これと同様の働きをする行列が**単位行列**であり，\n",
        "$$\n",
        "\\boldsymbol{I}=\\begin{bmatrix}\n",
        "1 & 0 & \\ldots  & 0 \\\\\n",
        "0 & 1 & \\ldots  & 0 \\\\\n",
        "\\vdots & \\vdots  & \\ddots  & \\vdots  \\\\\n",
        "0 & 0 & \\ldots  & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "のような形をしています．行列の斜めの要素を**対角要素**と呼び，それ以外の要素を非対角要素とよびます．単位行列は，対角要素が1で，非対角要素が0であるような行列です．例えば， $\\boldsymbol{I} \\in \\mathcal{R}^{2\\times 2}$ の場合，\n",
        "$$\n",
        "\\boldsymbol{I} =\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "であり，$\\boldsymbol{I} \\in \\mathcal{R}^{3\\times 3}$ の場合，\n",
        "$$\n",
        "\\boldsymbol{I}=\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "となります．\n",
        "\n",
        "実際に計算して，元の行列と値が変わらないかを確認してみると，\n",
        "$$\n",
        "\\begin{aligned}\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "&=\\begin{bmatrix}\n",
        "1\\times 1+2\\times 6 & 1\\times 0+2\\times 1 \\\\\n",
        "3\\times 1+4\\times 0 & 3\\times 0+4\\times 1\n",
        "\\end{bmatrix}\\\\\n",
        "&=\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "のように，確かに元の値と同じであることがわかりました．単位行列 $\\boldsymbol{I}$ は任意の行列$\\boldsymbol{A}$に対し\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{A}\\boldsymbol{I}&=\\boldsymbol{A}\\\\\n",
        "\\boldsymbol{I}\\boldsymbol{A}&=\\boldsymbol{A}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "が成り立ちます．\n",
        "\n",
        "### 逆行列\n",
        "\n",
        "**逆行列**とは\n",
        "$$\n",
        "2 \\times 2^{-1} = 1\n",
        "$$\n",
        "\n",
        "のようなスカラーの逆数に対応するような行列です．英語では **Inverse Matrix **とよびます．\n",
        "\n",
        "逆行列の定義は\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{A}\\boldsymbol{A}^{-1}=\\boldsymbol{I}\\\\\n",
        "\\boldsymbol{A}^{-1}\\boldsymbol{A}=\\boldsymbol{I}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "であり，ここで，$\\boldsymbol{I}$ は単位行列です．この逆行列も機械学習の計算過程に頻出するため覚えておきましょう．大学で習う線形代数の授業では，サイズが$2 \\times 2$ や $3 \\times 3$ であるような行列の逆行列の計算方法について習いますが，機械学習で扱う行列のサイズは $10 \\times 10, 1000 \\times 1000$ と大きく，逆行列を効率的に求める計算手法が提案されています．その計算方法については必ずしも知っておく必要はありませんが，どのような行列においても逆行列を計算できるわけではないため，逆行列が計算できるための条件は知っておきましょう．逆行列は**正方行列**と呼ばれる行と列のサイズが同じであるような行列でないと計算できません．\n",
        "\n",
        "もう一つの条件についてはそれを説明するのに必要な概念をまだ紹介していませんが，行列がフルランク（行列のランクが行数=列数）であること，これと等価で行列値が非0である必要があります．実は世の中の多くの行列はこれらを満たしませんが，対角成分に非ゼロの値を足すことで行列をフルランクにし，逆行列を計算できるようになります．もしアルゴリズムの中で逆行列を求める前に対角要素に小さな値を足すような操作があれば，それは逆行列を計算できるようにしていると思ってください．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/11.png)\n",
        "\n",
        "### 線形結合と二次形式\n",
        "\n",
        "機械学習の式によく出てくる形式として $\\boldsymbol{b}^{T}\\boldsymbol{x}$ と $\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}$ のような２形式があります．前者は*線形結合**もしくは**一次結合**，後者は**二次形式**と呼ばれています．中学校数学でいうところの，一次式や二次式の話と考えてもらうと分かりやすいと思います．\n",
        "\n",
        "線形結合の計算の中身を見てみると，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{b}&=\\begin{bmatrix}\n",
        "1 \\\\\n",
        "2\n",
        "\\end{bmatrix},\\ \n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\boldsymbol{b}^{T}\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "1 & 2\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}=x_{1}+2x_{2}\\end{aligned}\n",
        "$$\n",
        "\n",
        "のように $\\boldsymbol{x}$ の要素である $x_{1}$ もしくは $x_{2}$ に関して，一次式となっていることがわかります．\n",
        "\n",
        "また，二次形式も同様に計算の中身を確認してみると，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{A}&=\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix},\\ \n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}\n",
        "&=\\begin{bmatrix} x_{1} & x_{2}\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "3 & 4\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "&=\\begin{bmatrix}x_{1} & x_{2}\\end{bmatrix} \\begin{bmatrix}\n",
        "x_{1}+2x_{2} \\\\\n",
        "3x_{1}+4x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "&=x_{1}\\left( x_{1}+2x_{2}\\right) +x_{2}\\left( 3x_{1}+4x_{2}\\right) \\\\\n",
        "&=x^{2}_{1}+5x_{1}x_{2}+4x_{2}^{2}\\end{aligned}\n",
        "$$\n",
        "\n",
        "となり，各要素において二次式となっていることがわかります．\n",
        "\n",
        "そして，一般にこれらを足し合わせて，\n",
        "$$\n",
        "\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{b}^{T}\\boldsymbol{x} + c\n",
        "$$\n",
        "\n",
        "のように二次関数を表現します．ここで，$c$ はスカラーの定数項です．\n",
        "\n",
        "### 勾配　ベクトルで微分\n",
        "\n",
        "微分は入力を変えた場合の関数値の変化量と説明しました．同様に関数の入力がベクトルであり，ベクトルで微分をとることも考えることができます．\n",
        "\n",
        "機械学習で使われるのが勾配です．これは関数をそれぞれのベクトルの成分毎に偏微分を計算し，それらを並べてベクトルにしたものです．\n",
        "\n",
        "勾配の計算を紹介する前に，下記の例題を計算しましょう．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{b}&=\\begin{bmatrix}\n",
        "3 \\\\\n",
        "4\n",
        "\\end{bmatrix}, \\ \n",
        "\\boldsymbol{x}=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\boldsymbol{b}^{T}\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "3 & 4\n",
        "\\end{bmatrix}\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\n",
        "=3x_{1}+4x_{2}\\end{aligned}\n",
        "$$\n",
        "\n",
        "この計算は線形結合としてすでに紹介していますが，ここからが本題です．この $\\boldsymbol{b}^{T}\\boldsymbol{x}$ を ベクトル $\\boldsymbol{x}$ で微分したいというシチュエーションが機械学習の中では登場します．つまり，\n",
        "$$\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right)\n",
        "$$\n",
        "\n",
        "を求めたいというわけです．これを**ベクトルで微分**すると言います．ベクトルで微分をとる場合はそれぞれのベクトルの成分$x_1, x_2$毎に偏微分を計算し，それを並べます．今回の例の場合は\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right) &=\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( 3x_{1}+4x_{2}\\right) \\\\\n",
        "&=\\begin{bmatrix}\n",
        "\\dfrac {\\partial }{\\partial x_{1}} \\left( 3x_{1}+4x_{2}\\right)  \\\\\n",
        "\\dfrac {\\partial }{\\partial x_{2}} \\left( 3x_{1}+4x_{2}\\right) \n",
        "\\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "のように，ベクトルの要素（今回だと $x_1$と$x_{2}$）のそれぞれで偏微分した値をベクトルとして格納していくだけです．一見複雑そうに見えますが，シンプルな演算で構成されていることがわかります．実際に計算していくと，\n",
        "$$\n",
        "\\begin{aligned}\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}+4x_{2}\\right) &=\\dfrac {\\partial }{\\partial x_{1}}\\left( 3x_{1}\\right) +\\dfrac {\\partial }{\\partial x_{1}}\\left( 4x_{2}\\right) \\\\\n",
        "&=3\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( x_{1}\\right) +4x_{2}\\times \\dfrac {\\partial }{\\partial x_{1}}\\left( 1\\right) \\\\\n",
        "&=3\\times 1+4x_{2}\\times 0\\\\\n",
        "&=3\\end{aligned}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\\dfrac {\\partial }{\\partial x_{2}}\\left( 3x_{1}+4x_{2}\\right)&=\\dfrac {\\partial }{\\partial x_{2}}\\left( 3x_{1}\\right) +\\dfrac {\\partial }{\\partial x_{2}}\\left( 4x_{2}\\right) \\\\\n",
        "&=3x_{1}\\times \\dfrac {\\partial }{\\partial x_{2}}\\left( 1\\right) +4\\times \\dfrac {\\partial }{ax_{2}}\\left( x_{2}\\right) \\\\\n",
        "&=3x_{1} \\times 0 + 4 \\times 1 \\\\\n",
        "&= 4\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "となり，下記の計算結果が得られます．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right) \n",
        "&=\\begin{bmatrix}\n",
        "\\dfrac {\\partial }{\\partial x_{1}} \\left( 3x_{1}+4x_{2}\\right)  \\\\\n",
        "\\dfrac {\\partial }{\\partial x_{2}} \\left( 3x_{1}+4x_{2}\\right) \n",
        "\\end{bmatrix} =\\begin{bmatrix}\n",
        "3  \\\\\n",
        "4\n",
        "\\end{bmatrix} = \\boldsymbol{b}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "ここで，ベクトルで微分した結果が最初の $\\boldsymbol{b}$ と同じになっていることがわかります．これはたまたまではなく，一般的に成り立ちますが，この結果は最後に公式としてまとめましょう．\n",
        "\n",
        "もう一問，以下の例題を考えましょう．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\boldsymbol{x}&=\\begin{bmatrix}\n",
        "x_{1} \\\\\n",
        "x_{2}\n",
        "\\end{bmatrix}\\\\\n",
        "\\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( 1\\right) &=\\begin{bmatrix}\n",
        "\\dfrac {\\partial }{\\partial x_{1}}\\left( 1\\right)  \\\\\n",
        "\\dfrac {\\partial }{\\partial x_{2}}\\left( 1\\right) \n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "0 \\\\\n",
        "0\n",
        "\\end{bmatrix}=\\boldsymbol{0}\\end{aligned}\n",
        "$$\n",
        "\n",
        "もちろん，偏微分を行う対象の変数が含まれていない場合は 0 となります．要素が 0 のみで構成されたベクトルを**零（ゼロ）ベクトル**と言います．\n",
        "\n",
        "これらを踏まえて，公式としてまとめておきましょう．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\left( 1\\right) \\ \\dfrac {\\partial}{\\partial \\boldsymbol{x}}\\left( c\\right) = \\boldsymbol{0}\\\\\n",
        "&\\left( 2\\right) \\ \\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{b}^{T}\\boldsymbol{x}\\right) = \\boldsymbol{b}\\\\\n",
        "&\\left( 3\\right) \\ \\dfrac {\\partial }{\\partial \\boldsymbol{x}}\\left( \\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}\\right) =\\left( \\boldsymbol{A}+\\boldsymbol{A}^{T}\\right) \\boldsymbol{x}\\end{aligned}\n",
        "$$\n",
        "\n",
        "(1)と(2) に関してはすでに計算したものであり，(3) に関しては導出が少し複雑なので省略しますが，数値を代入して確認してみてください．こちらの 3 つの公式は機械学習を学んでいく上で非常に重要な公式となりますので，必ず覚えておきましょう．\n",
        "\n",
        "こういった行列などにおける公式は他にもたくさんあり，論文などを読む際にはどういった公式があるのかを知っておくことも重要です．その際に，私がよく用いる便利な公式集がネット上にあり，[The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) が無料で公開されており，おすすめです．\n",
        "\n",
        "\n",
        "\n",
        "## 統計\n",
        "\n",
        "### 確率や統計は何に使えるのか\n",
        "\n",
        "機械学習といえば確率や統計といったイメージで勉強する人も多いと思いますが，簡単なアルゴリズムであれば，微分と線形代数を理解しておくだけで説明することができ，確率や統計が出てくることはありません．それでは，確率，統計では何を行うことができるのでしょうか．\n",
        "\n",
        "それは，**データの分布の情報を定量評価できる**ことと，**データの分布を定式化できる**ことです．一見，同じように見えるかも知れませんが，目的がそれぞれ異なります．前者では，データの平均などといった情報を知ることで，アルゴリズム以前の**データ前処理**として使うことができます．後者では，分布の情報を定式化して**機械学習のモデル**に使用していきます．要するに使い道が前処理なのか，モデル化なのか，という違いです．前者の場合，学ぶべき数学が少なくて済むことに対し，後者の場合，**ベイズ統計**と呼ばれる生成モデルを取り扱うための数学が必要となり，多くの前提知識を必要とします．確率統計とひとまとめで呼ばれがちですが，前者が統計で，後者は確率として区別しています．\n",
        "\n",
        "ここでは，まずデータの前処理のための統計を学んでいきましょう．この統計を学ぶことによって，次の2つの問題に対して解決策を提示できるようになります．各入力変数の値が0～100であったり，0～1であったり，スケールがばらばらであり，アルゴリズム内部でデータを扱う際にそのスケールの差による悪影響を受ける場合があります．このスケールを揃える方法を考えます．また，データの**欠損値**は簡単に見つけることができるため取り除くのは難しくありませんが，データの**外れ値**は「どのようなデータが外れているのか」といった定義が必要になります．この**スケーリング**と**外れ値除去**は，データの前処理として実務ではほぼ必ず行うものであり，重要なパートといえます．\n",
        "\n",
        "### 統計量\n",
        "\n",
        "ここでは，よく使う統計量である平均と分散について紹介します．\n",
        "\n",
        "まずはじめに紹介するのは，最も有名な統計量である**平均**です．たとえば，300円, 400円, 500円の平均は，\n",
        "$$\n",
        "\\dfrac{300 + 400 + 500}{3} = 400\n",
        "$$\n",
        "\n",
        "となり，すべてを足し合わせて対象の数で割ります．これを定式化すると，\n",
        "$$\n",
        "\\begin{aligned}\\overline {x}=\\dfrac {x_{1}+x_{2}+\\ldots +x_{N}}{N}\n",
        "=\\dfrac {1}{N}\\sum ^{N}_{n=1}x_{n}\\end{aligned}\n",
        "$$\n",
        "\n",
        "のようになります．$n$ は**サンプルの数**を表すときの文字として使用します．平均は， $\\bar{x}$ や $\\mu$ といった記号で表わされるのが一般的です．データの分布においては，その重心に相当する値となります．\n",
        "\n",
        "次に，**分散**を紹介します．分散の定義は\n",
        "$$\n",
        "\\begin{aligned}\\sigma ^{2}=\\dfrac {1}{N}\\sum ^{N}_{n=1}\\left( x_{n}-\\overline {x}\\right) ^{2}\\end{aligned}\n",
        "$$\n",
        "\n",
        "です．各サンプルの平均 $\\bar{x}$ からの差分 $x- \\bar{x}$ を計算し，それが二乗誤差の場合と同様，正と負の値を持ってしまうため，二乗してすべてを正にしてから**総和**を取って，平均の値を計算しています．つまり，平均からどの程度離れているか（の二乗）の平均値といえます．分散にはもう一つ定義があり，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\sigma ^{2}=\\dfrac {1}{N-1}\\sum ^{N}_{n=1}\\left( x_{n}-\\overline {x}\\right) ^{2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "と表す場合もありあります．前者は**母分散**といい，後者は**不偏分散**といいます．これらの式の導出は他書に譲るとして，ここではその使い分けについて説明します．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/12.png)\n",
        "\n",
        "データ解析を行う際に，**母集団**に対する解析か**標本集団**に対する解析かを意識します．母集団とは解析を行いたい想定の範囲に対して，すべてのデータが揃っている場合であり，標本集団はそのうちの一部を抽出する場合です．例えば，全国の小学生の身長と体重を集計する場合に，全国の小学生を一人の抜け漏れもなく集められれば母集団であり，各都道府県で100人ずつ抜き出して集計する場合は標本集団です．母集団のデータを集めることは現実的に難しいことが多く，標本集団のデータから母集団の分布を推定することが一般的です．そうなると，基本的には標本集団向けである不偏分散を使用することになります．\n",
        "\n",
        "さて，分散についての定義がわかったところで，これは何に使えるのでしょうか．分散はデータのばらつきを定量評価することができます．実験をしたときに，このばらつきが多ければ，各実験で再現性を確保できていないことを見つけることができるように，何度か試行して平均に集まっていることが望ましい状況において，ざっくりと感覚で議論するのではなく，定量評価できることはとても助かります．また，0～1のスケールのデータでは，分散は小さな値になり，0～1000のスケールのデータでは，分散は大きな値になります．もちろん，そのデータのばらつき具合にもよりますが，分散を使えば**スケールの違い**も評価することができます．そのため，この情報はスケーリングに使えそうであることがわかります．\n",
        "\n",
        "最後に**標準偏差**です．分散では各サンプルの平均からの差の二乗の合計のため，単位は元の単位の二乗となっています．例えば元の単位がkgであれば，分散はkg*kgというスケールになります．これでは，分散が大きいかどうかを議論する場合に混乱してしまいます．そこで，元のスケールで議論するため，\n",
        "\n",
        "$$\n",
        "\\sigma = \\sqrt{\\sigma^{2}}\n",
        "$$\n",
        "\n",
        "のような分散の平方根を用いてばらつきをみます．これを標準偏差と呼びます．\n",
        "\n",
        "それでは，定義と使い道が見えてきたところで，練習問題で具体的な計算手順の確認を行いましょう．以下の①と②のデータに対して，平均，分散，標準偏差を求めてください．ただし，今回は母分散を使用することとします．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/13.png)\n",
        "\n",
        "①の解答は以下の通りです．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\bar{x}&=\\dfrac {1}{5}\\left( -2-1+0+1+2\\right) =0\\\\\n",
        "\\sigma ^{2}&=\\dfrac {1}{5}\\left\\{ \\left( -2-0\\right) ^{2}+\\left( -1-0\\right) ^{2}+(0-0)^{2}+(1-0)^{2}+(2-0)^{2}\\right\\} \\\\\n",
        "&=\\dfrac {1}{5}\\times 10=2\\\\\n",
        "\\sigma &=\\sqrt {2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "また，②の解答は以下の通りです．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\overline {x}&=\\dfrac {1}{5}\\left( -4-2+0+2+4\\right) =0\\\\\n",
        "\\sigma ^{2}&=\\dfrac {1}{5}\\left\\{ \\left( -4-0\\right) ^{2}+\\left( -2-0\\right) ^{2}+\\left( 0-0\\right) ^{2}+\\left( 2-0\\right) ^{2}+\\left( 4-0\\right) ^{2}\\right\\} \\\\\n",
        "&=\\dfrac {1}{5}\\times 40=8\\\\\n",
        "\\sigma &=\\sqrt {8}=2\\sqrt {2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "これより，②のケースの方が分散が大きく，データのばらつきが大きいことがわかります．\n",
        "\n",
        "### 正規分布\n",
        "\n",
        "確率では何度も登場する**正規分布**です．**ガウス分布**とも呼ばれています．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/14.png)\n",
        "\n",
        "では，なぜこの正規分布がよく登場するのでしょうか．その理由として，以下のような物理的・数学的背景があります．\n",
        "\n",
        "- 物理現象でこの分布に従うことがよくある\n",
        "- 数式が扱いやすい\n",
        "\n",
        "独立で多数の因子の和として表される確率変数は中心極限定理より正規分布に従うことが知られており，物理現象でこのような確率変数は多く扱われます．（一方で正規分布ではないような分布に対し正規分布にあてはめて考えてしまい誤った結論を導く場合も多々あります．）正規分布の数式が扱いやすいのは，正規分布は指数型分布族とよばれる形をしており，効率的な計算（尤度や事後確率分布など）ができるためです．\n",
        "\n",
        "正規分布では平均 $\\mu$ と標準偏差 $\\sigma$ に対して，何%がその分布に入っているかといった議論を良く行います．例えば，$\\mu \\pm 3\\sigma$ の範囲内に，データの全体の99.7%が入るため，この $\\mu \\pm 3 \\sigma$ に入らない領域を外れ値として定義するといった使い方をします．\n",
        "\n",
        "### スケーリング\n",
        "\n",
        "スケーリングはどのアルゴリズムでも前処理として重要になってきますが，ここでは簡単に２つの事例を紹介します．\n",
        "\n",
        "まず１つ目が距離の問題です．スケールが異なる変数 $x_{1}$ と $x_{2}$ があった場合に，下記の図のような状況になります．ここで，縦軸と横軸のスケールが大きく異なりますが，わざと同じようなスケールに見えるようにプロットしています．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/15.png)\n",
        "\n",
        "\n",
        "\n",
        "この２点間の距離 $d$ を求めると，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "d&=\\sqrt {\\left( 100-1000\\right) ^{2}+\\left( 0.1-1\\right) ^{2}}\\\\\n",
        "&= \\sqrt {900^{2}+0.9^{2}}\\\\\n",
        "&= \\sqrt {810000+0.81} \\\\\n",
        "&= \\sqrt {810000.81}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "のようになります．ここで着目したい点として，$x_{1}$ と $x_{2}$のどちらが距離 $d$ に対して影響を与えているかですが，明らかに $x_{1}$ です．$x_{2}$ に関しては，スケールが小さいがゆえにほとんど影響を与えていません．これでは $x_{2}$ がデータの意味として重要な場合においても考慮できません．こうした問題を解決する方法の一つが，ここで紹介する**スケーリング**です．代表的なスケーリングの方法としては２つあります．\n",
        "\n",
        "１つ目が，サンプル集合を**最小値0**，**最大値1**にスケーリングする方法です．これを**Min-Max スケーリング**と呼びます．この方法は単純で，各変数ごとに最小値 $x_{\\min}$ と最大値 $x_{\\max}$ を求めておき，すべてのデータに対して，\n",
        "$$\n",
        "\\widetilde{x} = \\dfrac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
        "$$\n",
        "の計算を行います．Min-Maxスケーリングには計算が単純というメリットがある反面，下図の例ように$x_1$で外れ値を持つデータ点が存在するような場合，$x_{\\max}$ が外れ値であるこの一点に大きく引っ張られてしまうという弱点があります．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/16.png)\n",
        "\n",
        "もう１つのスケーリングの方法として，**平均0**，**標準偏差1** にスケーリングする方法があります．分散含め，標準偏差ではデータのばらつきを定量評価することができ，\n",
        "$$\n",
        "\\widetilde{x}  = \\dfrac{x - \\bar{x}}{\\sigma}\n",
        "$$\n",
        "のように，標準偏差で割ることで，スケールを統一することができます．分散を計算した例題の①に対して，適用してみると，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "x_{1}&=\\dfrac {-2-0}{\\sqrt {2}}=-\\dfrac {2}{\\sqrt {2}}\\\\\n",
        "x_{2}&=\\dfrac {-1-0}{\\sqrt {2}}=-\\dfrac {1}{\\sqrt {2}}\\\\\n",
        "x_{3}&=\\dfrac {0-0}{\\sqrt {2}}=0\\\\\n",
        "x_{4}&=\\dfrac {1-0}{\\sqrt {2}}=\\dfrac {1}{\\sqrt {2}}\\\\\n",
        "x_{5}&=\\dfrac {2-0}{\\sqrt {2}}=\\dfrac {2}{\\sqrt {2}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "のように，データが変換されます．この時の平均と標準偏差を求めてみると，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\overline {x}&=\\dfrac {1}{5}\\left( -\\dfrac {2}{\\sqrt {2}}-\\dfrac {1}{\\sqrt {2}}+0+\\dfrac {1}{\\sqrt {2}}+\\dfrac {2}{\\sqrt {2}}\\right) =0\\\\\n",
        "\\sigma ^{2}&=\\dfrac {1}{5}\\left\\{ \\left( -\\dfrac {2}{\\sqrt {2}}-0\\right) ^{2}+\\left( -\\dfrac {1}{\\sqrt {2}}-0\\right) ^{2}+\\left( 0-0\\right) ^{2}\n",
        " +\\left( \\dfrac {1}{\\sqrt {2}}-0\\right) ^{2}+\\left( \\dfrac {2}{\\sqrt {2}}-0\\right) ^{2}\\right\\} =1\\\\\n",
        "\\sigma &=\\sqrt {\\sigma ^{2}}=1\n",
        "\\end{aligned}\n",
        "$$\n",
        "のように，平均0，標準偏差1にスケーリングできていることがわかります．この方法であれば，統計量を使用するため全体の傾向で議論することができ，１点だけの外れ値のようなケースには強い（これを**ロバスト**と表現する）と言えます．\n",
        "\n",
        "### 外れ値除去\n",
        "\n",
        "以下のように時間によって変動するようなデータを扱うとしましょう．例えば，横軸が時刻，縦軸がCPUの負荷率(%)と考えるとわかりやすいと思います．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/17.png)\n",
        "\n",
        "このデータに対して，CPUの負荷率の異常（外れ値）を検出したい場合，どのようにこの外れ値を定義して検出すれば良いでしょうか．その答えは，値の**頻度**に着目することです．\n",
        "\n",
        "![](https://github.com/japan-medical-ai/medical-ai-course-materials/raw/master/notebooks/images/1/18.png)\n",
        "\n",
        "上図のように，平均に対して線を引き，それぞれの値において頻度を算出してヒストグラムを描いてみると正規分布が現れます．物理現象として正規分布に従うものが多いと説明しましたが，このように中心付近の値の頻度は多く，離れるほど頻度が少なくなっていく現象では正規分布をあてはめることができます．そして，外れ値を定義するために，データの平均 $\\mu$ と標準偏差 $\\sigma$ を計算し，$\\mu \\pm 3\\sigma$の値に線を引けば，外れ値除去を行うことができます．これを**3σ法**と呼び，理論がシンプルかつ，プログラムの実装的にも平均と標準偏差だけで行えて簡単です．外れ値の回数が多い場合などは平均や標準偏差がその外れ値に引っ張られ，3σ法ではうまく対処できない場合があり，そのような場合には中央値をベースとした**Hampel判別法**を用います．\n",
        "\n",
        "この他，データを大きい順に並べてそれらの上位10%，下位10%を取り除くといったことも行われます．"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Basic Math for ML",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
